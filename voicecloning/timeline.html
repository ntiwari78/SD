<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Cloning Research Timeline (2023-2025)</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.3);
        }
        
        h1 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.5em;
        }
        
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            margin-bottom: 30px;
            font-size: 1.1em;
        }
        
        .stats {
            display: flex;
            justify-content: space-around;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }
        
        .stat-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px 25px;
            border-radius: 10px;
            text-align: center;
            margin: 5px;
            min-width: 150px;
        }
        
        .stat-number {
            font-size: 2em;
            font-weight: bold;
        }
        
        .stat-label {
            font-size: 0.9em;
            opacity: 0.9;
        }
        
        .legend {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-bottom: 30px;
        }
        
        .legend-item {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 8px 15px;
            background: #f8f9fa;
            border-radius: 20px;
            cursor: pointer;
            transition: transform 0.2s;
        }
        
        .legend-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        
        .legend-color {
            width: 20px;
            height: 20px;
            border-radius: 50%;
            border: 2px solid #333;
        }
        
        .timeline {
            position: relative;
            padding: 20px 0;
        }
        
        .year-section {
            margin-bottom: 50px;
        }
        
        .year-header {
            font-size: 2em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #3498db;
        }
        
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
        }
        
        .paper-card {
            background: white;
            border-radius: 10px;
            padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            border-left: 5px solid;
            transition: all 0.3s ease;
            cursor: pointer;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 15px rgba(0,0,0,0.2);
        }
        
        .paper-name {
            font-size: 1.3em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        
        .paper-date {
            font-size: 0.9em;
            color: #7f8c8d;
            margin-bottom: 10px;
        }
        
        .paper-category {
            display: inline-block;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.85em;
            color: white;
            margin-bottom: 10px;
        }
        
        .paper-desc {
            color: #555;
            line-height: 1.6;
            font-size: 0.95em;
        }
        
        .filter-buttons {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }
        
        .filter-btn {
            padding: 10px 20px;
            border: 2px solid #3498db;
            background: white;
            color: #3498db;
            border-radius: 25px;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: bold;
        }
        
        .filter-btn:hover, .filter-btn.active {
            background: #3498db;
            color: white;
        }
        
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è Voice Cloning Research Timeline</h1>
        <p class="subtitle">Comprehensive Survey of Zero-Shot and Controllable Speech Synthesis (2023-2025)</p>
        
        <div class="stats">
            <div class="stat-box">
                <div class="stat-number">44</div>
                <div class="stat-label">Papers Analyzed</div>
            </div>
            <div class="stat-box">
                <div class="stat-number">5</div>
                <div class="stat-label">Paradigms</div>
            </div>
            <div class="stat-box">
                <div class="stat-number">200k+</div>
                <div class="stat-label">Training Hours</div>
            </div>
            <div class="stat-box">
                <div class="stat-number">32</div>
                <div class="stat-label">Max Languages</div>
            </div>
        </div>
        
        <div class="filter-buttons">
            <button class="filter-btn active" onclick="filterPapers('all')">All Papers</button>
            <button class="filter-btn" onclick="filterPapers('Neural Codec LM')">Neural Codec LM</button>
            <button class="filter-btn" onclick="filterPapers('Flow/Diffusion')">Flow/Diffusion</button>
            <button class="filter-btn" onclick="filterPapers('Disentanglement')">Disentanglement</button>
            <button class="filter-btn" onclick="filterPapers('Controllable')">Controllable</button>
            <button class="filter-btn" onclick="filterPapers('Multimodal')">Multimodal</button>
        </div>
        
        <div class="legend">
            <div class="legend-item">
                <div class="legend-color" style="background: #3498db;"></div>
                <span>Neural Codec LM</span>
            </div>
            <div class="legend-item">
                <div class="legend-color" style="background: #e74c3c;"></div>
                <span>Flow/Diffusion</span>
            </div>
            <div class="legend-item">
                <div class="legend-color" style="background: #2ecc71;"></div>
                <span>Disentanglement</span>
            </div>
            <div class="legend-item">
                <div class="legend-color" style="background: #f39c12;"></div>
                <span>Controllable</span>
            </div>
            <div class="legend-item">
                <div class="legend-color" style="background: #9b59b6;"></div>
                <span>Multimodal</span>
            </div>
            <div class="legend-item">
                <div class="legend-color" style="background: #1abc9c;"></div>
                <span>Watermarking</span>
            </div>
        </div>
        
        <div class="timeline" id="timeline"></div>
    </div>
    
    <script>
        const papers = [
            // 2023
            {name: 'VALL-E', date: '2023-01', category: 'Neural Codec LM', desc: 'Pioneering zero-shot codec language model. First to demonstrate 3-second prompt voice cloning. Trained on 60,000 hours of Libri-Light using EnCodec with 8 RVQ layers. Autoregressive for first layer, non-autoregressive for subsequent layers.', year: 2023},
            {name: 'Voicebox', date: '2023-06', category: 'Flow/Diffusion', desc: 'Meta AI\'s breakthrough using optimal transport flow matching. Non-autoregressive model achieving 20√ó faster inference than VALL-E. Trained on 60k hours English + 50k multilingual. Enables zero-shot TTS, noise removal, and speech editing.', year: 2023},
            {name: 'Mega-TTS', date: '2023-03', category: 'Neural Codec LM', desc: 'Combined VQGAN acoustic modeling with latent language modeling. Achieved strong speaker similarity on VCTK and AISHELL-3 datasets. Trained on 20,000 hours of speech data.', year: 2023},
            {name: 'Mega-TTS 2', date: '2023-09', category: 'Neural Codec LM', desc: 'Enhanced version with multi-reference timbre encoder and prosody latent LM. Enables extraction of rich information from long prompts. Scaled training to 60-200k hours.', year: 2023},
            {name: 'HierSpeech++', date: '2023-07', category: 'Disentanglement', desc: 'Hierarchical framework with Text-to-Vec, dual audio encoders, and BigVGAN-based generation. Significantly improved zero-shot robustness and speaker similarity through hierarchical modeling.', year: 2023},
            {name: 'SSL-TTS', date: '2023-05', category: 'Disentanglement', desc: 'Leveraged self-supervised learning with HuBERT and wav2vec 2.0 representations. Separate conditioning for rhythm and timbre enabling rhythm transfer. Robust speaker embeddings without explicit modeling.', year: 2023},
            
            // 2024
            {name: 'VALL-E 2', date: '2024-02', category: 'Neural Codec LM', desc: 'Achieved "human parity" milestone. Key innovations: repetition-aware sampling (prevents infinite loop) and grouped code modeling (joint prediction of multiple codec tokens). Improved inference speed and quality.', year: 2024},
            {name: 'F5-TTS', date: '2024-05', category: 'Flow/Diffusion', desc: '"Embarrassingly simple" NAR architecture using flow matching with Diffusion Transformers (DiT). ConvNeXt-based text refinement and sway sampling at inference. Trained on 100,000 hours multilingual.', year: 2024},
            {name: 'E2 TTS', date: '2024-06', category: 'Flow/Diffusion', desc: 'Eliminated external aligners and explicit duration modeling via filler tokens. U-Net backbone handles generation. Simplified architecture while maintaining quality.', year: 2024},
            {name: 'NaturalSpeech 3', date: '2024-03', category: 'Flow/Diffusion', desc: 'Introduced factorized diffusion modeling content, prosody, timbre, and acoustic details in separate subspaces. Uses FACodec and factorized vector quantization. Quality on par with human recordings.', year: 2024},
            {name: 'MaskGCT', date: '2024-07', category: 'Neural Codec LM', desc: 'Masked Generative Codec Transformer predicting semantic and acoustic tokens via masked generation. Eliminates explicit alignment requirements. Trained on 100,000 hours.', year: 2024},
            {name: 'VoiceCraft', date: '2024-04', category: 'Multimodal', desc: 'Unified speech editing and zero-shot TTS using neural codec LM. Token rearrangement and causal masking. Introduced REAL-EDIT dataset for real-world evaluation.', year: 2024},
            {name: 'Seed-TTS', date: '2024-08', category: 'Neural Codec LM', desc: 'Family of versatile speech generation models with strong zero-shot performance. Foundation model approach enabling multiple speech generation tasks.', year: 2024},
            {name: 'MS-AP', date: '2024-03', category: 'Neural Codec LM', desc: 'Multi-Scale Acoustic Prompts augmented VALL-E with style and timbre prompts. Captures diverse speaker characteristics beyond short clips. Enhanced prompting mechanism.', year: 2024},
            {name: 'VALL-E R', date: '2024-09', category: 'Neural Codec LM', desc: 'Incorporated retrieval augmentation, retrieving similar speech segments to guide code prediction. Improved stability and expressiveness. Trained on LibriSpeech 960h.', year: 2024},
            {name: 'PALLE', date: '2024-10', category: 'Neural Codec LM', desc: 'Pseudo-autoregressive modeling bridging AR and NAR approaches. Reduces exposure bias while maintaining quality. Trained on LibriTTS dataset.', year: 2024},
            {name: 'CosyVoice', date: '2024-07', category: 'Multimodal', desc: 'Multilingual system (Chinese, English, Japanese, Korean, Cantonese) using supervised semantic tokens with conditional flow matching. Separates prosody and timbre for cross-lingual prompting. 170,000+ hours training.', year: 2024},
            {name: 'XTTS', date: '2024-05', category: 'Multimodal', desc: 'Neural codec LM trained across 16 languages with carefully curated data. Demonstrates cross-lingual TTS and code-switching capabilities.', year: 2024},
            {name: 'EmoSphere++', date: '2024-06', category: 'Controllable', desc: 'Emotion-controllable zero-shot TTS using emotion-adaptive spherical vector and multi-level style encoder. Separates speaker identity, prosody, and emotion. Evaluated on ESD and IEMOCAP.', year: 2024},
            {name: 'LiveSpeech 2', date: '2024-11', category: 'Flow/Diffusion', desc: 'Adapted zero-shot TTS to streaming scenarios with chunked context modeling and latency-aware decoding. Enables real-time low-latency voice cloning.', year: 2024},
            {name: 'ELaTE', date: '2024-08', category: 'Controllable', desc: 'Adapts flow-matching TTS to generate controlled laughter. Uses laughter detector and control embedding to synthesize laughing speech.', year: 2024},
            
            // 2025
            {name: 'DisCo-Speech', date: '2025-01', category: 'Disentanglement', desc: 'DisCodec with tri-factor disentanglement (content, prosody, timbre) at codec level. LM-based generator enables prosodic continuation with independent control. Trained on 26k hours, LM pre-trained on 120k+ hours multilingual.', year: 2025},
            {name: 'ControlSpeech', date: '2025-01', category: 'Controllable', desc: 'Simultaneous and independent control over speaker identity and style using natural language instructions. Disentangled representation space with encoder-decoder architecture and confidence-based codec generator.', year: 2025},
            {name: 'Spark-TTS', date: '2025-02', category: 'Neural Codec LM', desc: 'LLM-based framework with BiCodec (single-stream producing semantic tokens for content and global tokens for speaker attributes). Chain-of-thought generation. Released VoxBox benchmark: 102,500 hours annotated speech.', year: 2025},
            {name: 'FlexiVoice', date: '2025-01', category: 'Controllable', desc: 'Instruction-following style control with zero-shot cloning. LLM core with progressive post-training, direct preference optimization (DPO), and group relative policy optimization.', year: 2025},
            {name: 'Vevo', date: '2025-02', category: 'Disentanglement', desc: 'Two-stage framework for controllable voice imitation: AR transformer models content and style, flow-matching transformer handles acoustics. VQ-VAE disentangles timbre, style, content. Enables accent and emotion conversion.', year: 2025},
            {name: 'ZipVoice', date: '2025-01', category: 'Flow/Diffusion', desc: 'Flow-matching with Zipformer-based vector field estimation, average upsampling alignment, and flow distillation. 3√ó smaller and up to 30√ó faster while maintaining quality.', year: 2025},
            {name: 'MM-Sonate', date: '2025-02', category: 'Multimodal', desc: 'Unified multimodal framework for joint audio-video generation with zero-shot voice cloning. Multi-modal Diffusion Transformer (MM-DiT) with timbre injection mechanism and noise-based negative conditioning for CFG.', year: 2025},
            {name: 'VoiceMark', date: '2025-01', category: 'Watermarking', desc: 'Watermarking approach resistant to zero-shot voice cloning. Leverages speaker-specific latents as watermark carriers. RVQ models for latent disentanglement, cross-attention-based watermark embedders. High detection accuracy after synthesis.', year: 2025},
            {name: 'IndexTTS 2', date: '2025-01', category: 'Controllable', desc: 'Joint duration and emotion control using Qwen-based language model with soft instruction mechanisms. Trained on 55,000 sentences plus 135 hours emotional data from 361 speakers.', year: 2025},
            {name: 'IndexTTS 2.5', date: '2025-02', category: 'Multimodal', desc: 'Enhanced multilingual coverage (Chinese, English, Japanese, Spanish) with semantic codec compression, Zipformer-based architectures, and RL post-training. Achieves 2.28√ó faster inference.', year: 2025},
            {name: 'MiniMax-Speech', date: '2025-02', category: 'Multimodal', desc: 'Massive 32-language zero-shot TTS system with learnable speaker encoder. Demonstrates strong cross-lingual capabilities across diverse language families.', year: 2025},
            {name: 'TLA-SA', date: '2025-01', category: 'Disentanglement', desc: 'Time-Layer Adaptive Speaker Alignment addresses speaker similarity degradation in flow-matching TTS. Aligns speaker information across time steps and layers. Generalizes to different architectures.', year: 2025},
            {name: 'Voice Impression', date: '2025-01', category: 'Controllable', desc: 'Voice impression control on 11 antonym scales (dark-bright, rough-smooth, etc.). Collected 1,800 hours Japanese speech. Control module adjusts voice impression during synthesis.', year: 2025},
            {name: 'DS-TTS', date: '2025-01', category: 'Controllable', desc: 'Dynamic dual-style encoding networks separately capture global timbre and local prosody. Dynamic generator networks adapt to different sentence lengths.', year: 2025},
            {name: 'INTP', date: '2025-01', category: 'Controllable', desc: 'Preference alignment framework with Intelligibility Preference Speech Dataset (INTP): 250,000 preference pairs over 2,000 hours. DPO aligns TTS output with human preferences, improving intelligibility while preserving naturalness.', year: 2025},
            {name: 'MARS6', date: '2025-02', category: 'Multimodal', desc: 'Small hierarchical-codec architecture handling complex text inputs and expressive references more effectively than larger models. Compact and robust design.', year: 2025},
        ];
        
        const categoryColors = {
            'Neural Codec LM': '#3498db',
            'Flow/Diffusion': '#e74c3c',
            'Disentanglement': '#2ecc71',
            'Controllable': '#f39c12',
            'Multimodal': '#9b59b6',
            'Watermarking': '#1abc9c'
        };
        
        let currentFilter = 'all';
        
        function renderTimeline() {
            const timeline = document.getElementById('timeline');
            timeline.innerHTML = '';
            
            const years = [2023, 2024, 2025];
            
            years.forEach(year => {
                const yearPapers = papers.filter(p => p.year === year && 
                    (currentFilter === 'all' || p.category === currentFilter));
                
                if (yearPapers.length === 0) return;
                
                const yearSection = document.createElement('div');
                yearSection.className = 'year-section';
                
                const yearHeader = document.createElement('div');
                yearHeader.className = 'year-header';
                yearHeader.textContent = `${year} (${yearPapers.length} papers)`;
                yearSection.appendChild(yearHeader);
                
                const papersGrid = document.createElement('div');
                papersGrid.className = 'papers-grid';
                
                yearPapers.forEach(paper => {
                    const card = document.createElement('div');
                    card.className = 'paper-card';
                    card.style.borderLeftColor = categoryColors[paper.category];
                    
                    card.innerHTML = `
                        <div class="paper-name">${paper.name}</div>
                        <div class="paper-date">üìÖ ${paper.date}</div>
                        <div class="paper-category" style="background: ${categoryColors[paper.category]}">
                            ${paper.category}
                        </div>
                        <div class="paper-desc">${paper.desc}</div>
                    `;
                    
                    papersGrid.appendChild(card);
                });
                
                yearSection.appendChild(papersGrid);
                timeline.appendChild(yearSection);
            });
        }
        
        function filterPapers(category) {
            currentFilter = category;
            
            // Update button states
            document.querySelectorAll('.filter-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            event.target.classList.add('active');
            
            renderTimeline();
        }
        
        // Initial render
        renderTimeline();
    </script>
</body>
</html>
