<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Cloning Research Bibliography (2023-2025)</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 50%, #7e22ce 100%);
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1600px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        
        header {
            text-align: center;
            margin-bottom: 40px;
            padding-bottom: 30px;
            border-bottom: 3px solid #e0e0e0;
        }
        
        h1 {
            color: #1e3c72;
            font-size: 2.8em;
            margin-bottom: 10px;
            background: linear-gradient(135deg, #1e3c72, #7e22ce);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .subtitle {
            color: #666;
            font-size: 1.2em;
            margin-bottom: 20px;
        }
        
        .stats-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 40px;
        }
        
        .stat-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.3);
            transition: transform 0.3s;
        }
        
        .stat-card:hover {
            transform: translateY(-5px);
        }
        
        .stat-number {
            font-size: 3em;
            font-weight: bold;
            display: block;
            margin-bottom: 5px;
        }
        
        .stat-label {
            font-size: 1em;
            opacity: 0.95;
        }
        
        .controls {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            margin-bottom: 30px;
        }
        
        .search-box {
            width: 100%;
            padding: 15px 20px;
            font-size: 1.1em;
            border: 2px solid #ddd;
            border-radius: 10px;
            margin-bottom: 20px;
            transition: border-color 0.3s;
        }
        
        .search-box:focus {
            outline: none;
            border-color: #667eea;
            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
        }
        
        .filter-section {
            margin-bottom: 20px;
        }
        
        .filter-label {
            font-weight: bold;
            color: #333;
            margin-bottom: 10px;
            display: block;
            font-size: 1.1em;
        }
        
        .filter-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .filter-btn {
            padding: 10px 20px;
            border: 2px solid #667eea;
            background: white;
            color: #667eea;
            border-radius: 25px;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: 600;
            font-size: 0.95em;
        }
        
        .filter-btn:hover {
            background: #667eea;
            color: white;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
        }
        
        .filter-btn.active {
            background: #667eea;
            color: white;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        
        .sort-controls {
            display: flex;
            align-items: center;
            gap: 15px;
            flex-wrap: wrap;
        }
        
        .sort-select {
            padding: 10px 15px;
            border: 2px solid #ddd;
            border-radius: 8px;
            font-size: 1em;
            cursor: pointer;
            background: white;
        }
        
        .results-info {
            padding: 15px 20px;
            background: #e8f4f8;
            border-left: 4px solid #667eea;
            border-radius: 8px;
            margin-bottom: 20px;
            font-size: 1.05em;
            color: #333;
        }
        
        .papers-container {
            display: grid;
            gap: 25px;
        }
        
        .paper-card {
            background: white;
            border: 2px solid #e0e0e0;
            border-radius: 15px;
            padding: 25px;
            transition: all 0.3s;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
        }
        
        .paper-card:hover {
            border-color: #667eea;
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.2);
            transform: translateY(-3px);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 15px;
            gap: 20px;
        }
        
        .paper-title {
            font-size: 1.5em;
            font-weight: bold;
            color: #1e3c72;
            flex: 1;
            line-height: 1.3;
        }
        
        .paper-year {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 1em;
            white-space: nowrap;
        }
        
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin-bottom: 15px;
            align-items: center;
        }
        
        .paper-category {
            display: inline-block;
            padding: 6px 14px;
            border-radius: 15px;
            font-size: 0.9em;
            font-weight: 600;
            color: white;
        }
        
        .category-codec { background: #3498db; }
        .category-flow { background: #e74c3c; }
        .category-disentangle { background: #2ecc71; }
        .category-controllable { background: #f39c12; }
        .category-multimodal { background: #9b59b6; }
        .category-watermark { background: #1abc9c; }
        .category-foundation { background: #34495e; }
        .category-dataset { background: #16a085; }
        .category-ssl { background: #e67e22; }
        .category-technical { background: #95a5a6; }
        
        .paper-venue {
            color: #666;
            font-style: italic;
            font-size: 0.95em;
        }
        
        .paper-authors {
            color: #555;
            margin-bottom: 12px;
            font-size: 1em;
            line-height: 1.5;
        }
        
        .paper-links {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin-top: 15px;
        }
        
        .paper-link {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: #667eea;
            color: white;
            text-decoration: none;
            border-radius: 8px;
            font-size: 0.9em;
            font-weight: 600;
            transition: all 0.3s;
        }
        
        .paper-link:hover {
            background: #5568d3;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        
        .paper-link.doi {
            background: #2ecc71;
        }
        
        .paper-link.doi:hover {
            background: #27ae60;
        }
        
        .export-buttons {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            justify-content: flex-end;
        }
        
        .export-btn {
            padding: 10px 20px;
            background: #2ecc71;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s;
        }
        
        .export-btn:hover {
            background: #27ae60;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(46, 204, 113, 0.3);
        }
        
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: #666;
            font-size: 1.3em;
        }
        
        .hidden {
            display: none !important;
        }
        
        .year-group {
            margin-bottom: 40px;
        }
        
        .year-header {
            font-size: 2em;
            font-weight: bold;
            color: #1e3c72;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .year-count {
            font-size: 0.7em;
            color: #666;
            font-weight: normal;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 2em;
            }
            
            .paper-header {
                flex-direction: column;
            }
            
            .filter-buttons {
                justify-content: center;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üìö Voice Cloning Research Bibliography</h1>
            <p class="subtitle">Comprehensive Reference Collection (2023-2025)</p>
        </header>
        
        <div class="stats-container">
            <div class="stat-card">
                <span class="stat-number" id="totalPapers">0</span>
                <span class="stat-label">Total Papers</span>
            </div>
            <div class="stat-card">
                <span class="stat-number" id="year2023">0</span>
                <span class="stat-label">2023 Papers</span>
            </div>
            <div class="stat-card">
                <span class="stat-number" id="year2024">0</span>
                <span class="stat-label">2024 Papers</span>
            </div>
            <div class="stat-card">
                <span class="stat-number" id="year2025">0</span>
                <span class="stat-label">2025 Papers</span>
            </div>
            <div class="stat-card">
                <span class="stat-number" id="foundational">0</span>
                <span class="stat-label">Foundational</span>
            </div>
        </div>
        
        <div class="controls">
            <input type="text" id="searchBox" class="search-box" placeholder="üîç Search by title, authors, venue, or keywords...">
            
            <div class="filter-section">
                <label class="filter-label">Filter by Year:</label>
                <div class="filter-buttons" id="yearFilters">
                    <button class="filter-btn active" onclick="filterYear('all')">All Years</button>
                    <button class="filter-btn" onclick="filterYear('2023')">2023</button>
                    <button class="filter-btn" onclick="filterYear('2024')">2024</button>
                    <button class="filter-btn" onclick="filterYear('2025')">2025</button>
                    <button class="filter-btn" onclick="filterYear('foundational')">Foundational</button>
                </div>
            </div>
            
            <div class="filter-section">
                <label class="filter-label">Filter by Category:</label>
                <div class="filter-buttons" id="categoryFilters">
                    <button class="filter-btn active" onclick="filterCategory('all')">All Categories</button>
                    <button class="filter-btn" onclick="filterCategory('codec')">Neural Codec LM</button>
                    <button class="filter-btn" onclick="filterCategory('flow')">Flow/Diffusion</button>
                    <button class="filter-btn" onclick="filterCategory('disentangle')">Disentanglement</button>
                    <button class="filter-btn" onclick="filterCategory('controllable')">Controllable</button>
                    <button class="filter-btn" onclick="filterCategory('multimodal')">Multimodal</button>
                    <button class="filter-btn" onclick="filterCategory('dataset')">Datasets</button>
                    <button class="filter-btn" onclick="filterCategory('technical')">Technical/Foundation</button>
                </div>
            </div>
            
            <div class="filter-section">
                <label class="filter-label">Sort by:</label>
                <div class="sort-controls">
                    <select id="sortSelect" class="sort-select" onchange="sortPapers()">
                        <option value="year-desc">Year (Newest First)</option>
                        <option value="year-asc">Year (Oldest First)</option>
                        <option value="title">Title (A-Z)</option>
                        <option value="authors">First Author (A-Z)</option>
                    </select>
                </div>
            </div>
            
            <div class="export-buttons">
                <button class="export-btn" onclick="exportBibTeX()">üì• Export BibTeX</button>
                <button class="export-btn" onclick="exportJSON()">üì• Export JSON</button>
            </div>
        </div>
        
        <div class="results-info" id="resultsInfo"></div>
        
        <div id="papersContainer"></div>
        
        <div class="no-results hidden" id="noResults">
            <p>No papers match your current filters. Try adjusting your search criteria.</p>
        </div>
    </div>
    
    <script>
        const papers = [
            // 2023 Papers
            {
                id: 'wang2023valle',
                title: 'VALL-E: Neural Codec Language Models are Zero-Shot Text-to-Speech Synthesizers',
                authors: 'Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and He, Lei and Zhao, Sheng and Wei, Furu',
                year: 2023,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2301.02111',
                doi: '10.48550/arXiv.2301.02111',
                url: 'https://arxiv.org/abs/2301.02111'
            },
            {
                id: 'le2023voicebox',
                title: 'Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale',
                authors: 'Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Gopalani, Pascal and Hsu, Wei-Ning and Kreissig, Felix and Dhulipala, Laxman and Nachmani, Eliya and Levy, Andrew S',
                year: 2023,
                venue: 'NeurIPS 2023',
                category: 'flow',
                url: 'https://proceedings.neurips.cc/paper_files/paper/2023'
            },
            {
                id: 'lee2023hierspeech',
                title: 'HierSpeech++: Bridging the Gap between Semantic and Acoustic Representations for Zero-shot Speech Synthesis',
                authors: 'Lee, Sang-Hoon and Kim, Seung-Bin and Park, Jihyun and Lee, Seong-Whan',
                year: 2023,
                venue: 'arXiv preprint',
                category: 'disentangle',
                arxiv: '2311.12454',
                doi: '10.48550/arXiv.2311.12454',
                url: 'https://arxiv.org/abs/2311.12454'
            },
            {
                id: 'wu2023sgpss',
                title: 'Speaker-Guided Parallel Subnet Selection for Few-Shot Voice Cloning',
                authors: 'Wu, Y. and others',
                year: 2023,
                venue: 'Interspeech 2023',
                category: 'codec'
            },
            {
                id: 'fujita2023ssl',
                title: 'Zero-shot Text-to-Speech Synthesis Using Self-Supervised Speech Representations',
                authors: 'Fujita, Kenichi and others',
                year: 2023,
                venue: 'ICASSP Workshop 2023',
                category: 'ssl'
            },
            {
                id: 'jiang2023mega',
                title: 'Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias',
                authors: 'Jiang, Ziyue and others',
                year: 2023,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2306.03509',
                doi: '10.48550/arXiv.2306.03509',
                url: 'https://arxiv.org/abs/2306.03509'
            },
            {
                id: 'jiang2023megatts2',
                title: 'Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis',
                authors: 'Jiang, Ziyue and others',
                year: 2023,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2307.07218',
                doi: '10.48550/arXiv.2307.07218',
                url: 'https://arxiv.org/abs/2307.07218'
            },
            
            // 2024 Papers
            {
                id: 'chen2024f5',
                title: 'F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching',
                authors: 'Chen, Yushen and Niu, Zhikang and Ma, Ziyang and Deng, Keqi and Wang, Chunhui and Zhao, Jian and Yu, Kai and Chen, Xie',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'flow',
                arxiv: '2410.06885',
                doi: '10.48550/arXiv.2410.06885',
                url: 'https://arxiv.org/abs/2410.06885'
            },
            {
                id: 'ju2024naturalspeech3',
                title: 'NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models',
                authors: 'Ju, Zeqian and Shen, Kai and Tan, Xu and Liu, Yanqing and Leng, Yi-Chong and He, Lei and Wu, Zhizheng and Zhao, Sheng and Bian, Jiang',
                year: 2024,
                venue: 'ICML 2024',
                category: 'flow',
                arxiv: '2403.03100',
                doi: '10.48550/arXiv.2403.03100',
                url: 'https://arxiv.org/abs/2403.03100'
            },
            {
                id: 'eskimez2024e2',
                title: 'E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS',
                authors: 'Eskimez, Sefik Emre and Wang, Xiaofei and Thakker, Manthan and Li, Canrun and Kanda, Naoyuki and Zhu, Zirui and Tang, Min and Liu, Shujie and Li, Jinyu and Zhao, Sheng',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'flow',
                arxiv: '2406.18009',
                doi: '10.1109/SLT61566.2024.10832320',
                url: 'https://arxiv.org/abs/2406.18009'
            },
            {
                id: 'wang2024valle2',
                title: 'VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers',
                authors: 'Wang, Chengyi and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2406.05370',
                doi: '10.48550/arXiv.2406.05370',
                url: 'https://arxiv.org/abs/2406.05370'
            },
            {
                id: 'peng2024voicecraft',
                title: 'VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild',
                authors: 'Peng, Puyuan and Baade, Alan and Harwath, David',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'multimodal',
                arxiv: '2403.16973',
                doi: '10.48550/arXiv.2403.16973',
                url: 'https://arxiv.org/abs/2403.16973'
            },
            {
                id: 'wang2024msap',
                title: 'Multi-Scale Acoustic Prompts for Zero-Shot Speech Synthesis',
                authors: 'Wang, Chengyi and others',
                year: 2024,
                venue: 'ICASSP 2024',
                category: 'codec'
            },
            {
                id: 'ye2024maskgct',
                title: 'MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer',
                authors: 'Ye, Shaojun and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2409.00750',
                doi: '10.48550/arXiv.2409.00750',
                url: 'https://arxiv.org/abs/2409.00750'
            },
            {
                id: 'zhang2024cosy',
                title: 'CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer Based on Supervised Semantic Tokens',
                authors: 'Zhang, Chen and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'multimodal',
                arxiv: '2407.05407',
                doi: '10.48550/arXiv.2407.05407',
                url: 'https://arxiv.org/abs/2407.05407'
            },
            {
                id: 'sun2024xtts',
                title: 'XTTS: A Massively Multilingual Zero-Shot Text-to-Speech Model',
                authors: 'Sun, Guangzhi and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'multimodal',
                arxiv: '2406.04904',
                doi: '10.48550/arXiv.2406.04904',
                url: 'https://arxiv.org/abs/2406.04904'
            },
            {
                id: 'dang2024livespeech2',
                title: 'LiveSpeech 2: Towards Real-time Zero-shot Text-to-Speech for Continuous Text Streams',
                authors: 'Dang, Trung and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'flow',
                arxiv: '2410.00767',
                doi: '10.48550/arXiv.2410.00767',
                url: 'https://arxiv.org/abs/2410.00767'
            },
            {
                id: 'hayashi2024lightweight',
                title: 'Lightweight Zero-Shot Text-to-Speech with Mixture of Adapters',
                authors: 'Hayashi, Yuta and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2407.01291',
                doi: '10.48550/arXiv.2407.01291',
                url: 'https://arxiv.org/abs/2407.01291'
            },
            {
                id: 'xing2024palle',
                title: 'PALLE: Pseudo-Autoregressive Neural Codec Language Model for Zero-Shot TTS',
                authors: 'Xing, Jingyuan and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2404.10352',
                doi: '10.48550/arXiv.2404.10352',
                url: 'https://arxiv.org/abs/2404.10352'
            },
            {
                id: 'liu2024valleR',
                title: 'VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment',
                authors: 'Liu, Yang and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2406.07855',
                doi: '10.48550/arXiv.2406.07855',
                url: 'https://arxiv.org/abs/2406.07855'
            },
            {
                id: 'liu2024contexttts',
                title: 'Improving Audio-Codec-Based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context',
                authors: 'Liu, Yang and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2406.03706',
                doi: '10.48550/arXiv.2406.03706',
                url: 'https://arxiv.org/abs/2406.03706'
            },
            {
                id: 'seed2024seedtts',
                title: 'Seed-TTS: A Family of High-Quality Versatile Speech Generation Models',
                authors: 'ByteDance Seed Team',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'codec',
                arxiv: '2406.02430',
                doi: '10.48550/arXiv.2406.02430',
                url: 'https://arxiv.org/abs/2406.02430'
            },
            {
                id: 'lee2024ssltts',
                title: 'SSL-TTS: Leveraging Self-Supervised Embeddings and kNN Retrieval for Zero-Shot Multi-speaker TTS',
                authors: 'Lee, Kyungwoo and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'ssl',
                arxiv: '2408.10771',
                doi: '10.48550/arXiv.2408.10771',
                url: 'https://arxiv.org/abs/2408.10771'
            },
            {
                id: 'ling2024emosphere',
                title: 'EmoSphere++: Emotion-Controllable Zero-Shot Text-to-Speech via Emotion-Adaptive Spherical Vector',
                authors: 'Ling, Ziqi and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'controllable',
                arxiv: '2411.02625',
                doi: '10.48550/arXiv.2411.02625',
                url: 'https://arxiv.org/abs/2411.02625'
            },
            {
                id: 'huang2024covoc',
                title: 'A Two-Stage Autoregressive Codec TTS System with Classifier-Free Guidance for CoVoC Challenge',
                authors: 'Huang, Shengnan and others',
                year: 2024,
                venue: 'ISCSLP CoVoC Challenge 2024',
                category: 'codec'
            },
            {
                id: 'ju2024elate',
                title: 'ELaTE: Emotion-controllable Laughter Generation with Flow Matching',
                authors: 'Ju, Yue and others',
                year: 2024,
                venue: 'arXiv preprint',
                category: 'controllable',
                arxiv: '2403.03064',
                doi: '10.48550/arXiv.2403.03064',
                url: 'https://arxiv.org/abs/2403.03064'
            },
            
            // 2025 Papers
            {
                id: 'li2025discospeech',
                title: 'DisCo-Speech: Controllable Zero-Shot Speech Generation with a Disentangled Speech Codec',
                authors: 'Li, Tao and Huang, Jinwei and Yang, Runyan and Zhang, Shilei and Feng, Junlan',
                year: 2025,
                venue: 'EMNLP 2025',
                category: 'disentangle'
            },
            {
                id: 'ji2025controlspeech',
                title: 'ControlSpeech: Towards Simultaneous and Independent Zero-shot Speaker and Style Control',
                authors: 'Ji, Heng and others',
                year: 2025,
                venue: 'ACL 2025',
                category: 'controllable'
            },
            {
                id: 'qiang2025mmsonate',
                title: 'MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning',
                authors: 'Qiang, Chunyu and Wang, Jun and Wang, Xiaopeng and Yin, Kang and Guo, Yuxin and Zeng, Xijuan and Li, Nan and Li, Zihan and Liang, Yuzhe and Zhang, Ziyu and Ma, Teng and Chen, Yushen and Liu, Zhongliang and Deng, Feng and Zhang, Chen and Wan, Pengfei',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'multimodal',
                arxiv: '2601.01568',
                doi: '10.48550/arXiv.2601.01568',
                url: 'https://arxiv.org/abs/2601.01568'
            },
            {
                id: 'li2025indextts25',
                title: 'IndexTTS 2.5 Technical Report',
                authors: 'Li, Yunpei and others',
                year: 2025,
                venue: 'Bilibili Inc. Technical Report',
                category: 'multimodal'
            },
            {
                id: 'baas2025mars6',
                title: 'MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model',
                authors: 'Baas, Matthew and Scholtz, Pieter and Mehta, Arnav and Dyson, Elliott and Prakash, Akshat and Kamper, Herman',
                year: 2025,
                venue: 'ICASSP 2025',
                category: 'codec'
            },
            {
                id: 'li2025voicemark',
                title: 'VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents',
                authors: 'Li, Haiyun and Wu, Zhiyong and Xie, Xiaofeng and Xie, Jingran and Xu, Yaoxun and Peng, Hanyang',
                year: 2025,
                venue: 'Interspeech 2025',
                category: 'watermark'
            },
            {
                id: 'chen2025flexivoice',
                title: 'FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions',
                authors: 'Chen, Dekun and others',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'controllable',
                arxiv: '2601.01460',
                doi: '10.48550/arXiv.2601.01460',
                url: 'https://arxiv.org/abs/2601.01460'
            },
            {
                id: 'shen2025dstts',
                title: 'DS-TTS: Dynamic Dual-Style Zero-Shot Text-to-Speech',
                authors: 'Shen, Yao and others',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'controllable',
                arxiv: '2506.01020',
                doi: '10.48550/arXiv.2506.01020',
                url: 'https://arxiv.org/abs/2506.01020'
            },
            {
                id: 'katsumata2025zipvoice',
                title: 'ZipVoice: Flow-Matching Zero-Shot TTS with Zipformer and Flow Distillation',
                authors: 'Katsumata, Genta and others',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'flow',
                arxiv: '2503.04397',
                doi: '10.48550/arXiv.2503.04397',
                url: 'https://arxiv.org/abs/2503.04397'
            },
            {
                id: 'shen2025intelligibility',
                title: 'Advancing Zero-Shot TTS Intelligibility via Preference Alignment',
                authors: 'Shen, Yao and others',
                year: 2025,
                venue: 'ACL 2025',
                category: 'controllable'
            },
            {
                id: 'li2025vevo',
                title: 'Vevo: Controllable Zero-shot Voice Imitation with Self-Supervised Disentanglement',
                authors: 'Li, Yinpeng and others',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'disentangle',
                arxiv: '2502.07243',
                doi: '10.48550/arXiv.2502.07243',
                url: 'https://arxiv.org/abs/2502.07243'
            },
            {
                id: 'lee2025voiceimpression',
                title: 'Voice Impression Control in Zero-Shot Text-to-Speech',
                authors: 'Lee, Sungbin and others',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'controllable',
                arxiv: '2506.05688',
                doi: '10.48550/arXiv.2506.05688',
                url: 'https://arxiv.org/abs/2506.05688'
            },
            {
                id: 'zhou2025indextts2',
                title: 'IndexTTS2: Zero-Shot Text-to-Speech with Duration Control and Emotional Expression',
                authors: 'Zhou, Siyi and others',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'controllable',
                arxiv: '2506.21619',
                doi: '10.48550/arXiv.2506.21619',
                url: 'https://arxiv.org/abs/2506.21619'
            },
            {
                id: 'jiang2025tlasa',
                title: 'Time-Layer Adaptive Speaker Alignment for Zero-Shot Flow-Matching TTS',
                authors: 'Jiang, Haibo and others',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'disentangle',
                arxiv: '2504.06283',
                doi: '10.48550/arXiv.2504.06283',
                url: 'https://arxiv.org/abs/2504.06283'
            },
            {
                id: 'minimax2025speech',
                title: 'MiniMax-Speech: Multilingual Zero-Shot Text-to-Speech with Learnable Speaker Encoder',
                authors: 'MiniMax AI',
                year: 2025,
                venue: 'arXiv preprint',
                category: 'multimodal',
                arxiv: '2505.07916',
                doi: '10.48550/arXiv.2505.07916',
                url: 'https://arxiv.org/abs/2505.07916'
            },
            {
                id: 'wang2025sparktts',
                title: 'Spark-TTS: An LLM-Based Efficient Zero-Shot Voice Cloning TTS via Single-Stream Decoupled Speech Tokens',
                authors: 'Wang, Xinsheng and others',
                year: 2025,
                venue: 'ACL 2025',
                category: 'codec'
            },
            
            // Foundational Papers
            {
                id: 'defossez2022encodec',
                title: 'EnCodec: High Fidelity Neural Audio Compression',
                authors: 'D√©fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi',
                year: 2022,
                venue: 'arXiv preprint',
                category: 'technical',
                arxiv: '2210.13438',
                doi: '10.48550/arXiv.2210.13438',
                url: 'https://arxiv.org/abs/2210.13438',
                foundational: true
            },
            {
                id: 'kong2020hifigan',
                title: 'HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis',
                authors: 'Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung',
                year: 2020,
                venue: 'NeurIPS 2020',
                category: 'technical',
                arxiv: '2010.05646',
                doi: '10.48550/arXiv.2010.05646',
                url: 'https://arxiv.org/abs/2010.05646',
                foundational: true
            },
            {
                id: 'lee2022bigvgan',
                title: 'BigVGAN: A Universal Neural Vocoder with Large-Scale Training',
                authors: 'Lee, Sang-gil and Ping, Wei and Ginsburg, Boris and Catanzaro, Bryan and Yoon, Sungroh',
                year: 2022,
                venue: 'arXiv preprint',
                category: 'technical',
                arxiv: '2206.04658',
                doi: '10.48550/arXiv.2206.04658',
                url: 'https://arxiv.org/abs/2206.04658',
                foundational: true
            },
            {
                id: 'lipman2022flow',
                title: 'Flow Matching for Generative Modeling',
                authors: 'Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt',
                year: 2022,
                venue: 'arXiv preprint',
                category: 'technical',
                arxiv: '2210.02747',
                doi: '10.48550/arXiv.2210.02747',
                url: 'https://arxiv.org/abs/2210.02747',
                foundational: true
            },
            {
                id: 'zen2019libritts',
                title: 'LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech',
                authors: 'Zen, Heiga and Dang, Viet and Clark, Rob and Zhang, Yu and Weiss, Ron J and Jia, Ye and Chen, Zhifeng and Wu, Yonghui',
                year: 2019,
                venue: 'Interspeech 2019',
                category: 'dataset',
                doi: '10.21437/Interspeech.2019-2441',
                url: 'https://www.isca-speech.org/archive/interspeech_2019/zen19_interspeech.html',
                foundational: true
            },
            {
                id: 'kahn2020librilight',
                title: 'Libri-Light: A Benchmark for ASR with Limited or No Supervision',
                authors: 'Kahn, Jacob and Rivi√®re, Morgane and Zheng, Weiyi and Kharitonov, Evgeny and Xu, Qiantong and Mazar√©, Pierre-Emmanuel and Karadayi, Julien and Liptchinsky, Vitaliy and Collobert, Ronan and Fuegen, Christian and others',
                year: 2020,
                venue: 'arXiv preprint',
                category: 'dataset',
                arxiv: '1912.07875',
                doi: '10.48550/arXiv.1912.07875',
                url: 'https://arxiv.org/abs/1912.07875',
                foundational: true
            },
            {
                id: 'veaux2017vctk',
                title: 'CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit',
                authors: 'Veaux, Christophe and Yamagishi, Junichi and MacDonald, Kirsten',
                year: 2017,
                venue: 'University of Edinburgh',
                category: 'dataset',
                doi: '10.7488/ds/2645',
                url: 'https://doi.org/10.7488/ds/2645',
                foundational: true
            },
            {
                id: 'cao2020esd',
                title: 'Emotional Speech Dataset (ESD) for Speech Synthesis and Voice Conversion',
                authors: 'Cao, Kun and others',
                year: 2020,
                venue: 'arXiv preprint',
                category: 'dataset',
                arxiv: '2010.14794',
                doi: '10.48550/arXiv.2010.14794',
                url: 'https://arxiv.org/abs/2010.14794',
                foundational: true
            },
            {
                id: 'busso2008iemocap',
                title: 'IEMOCAP: Interactive Emotional Dyadic Motion Capture Database',
                authors: 'Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S',
                year: 2008,
                venue: 'Language Resources and Evaluation',
                category: 'dataset',
                doi: '10.1007/s10579-008-9076-6',
                url: 'https://doi.org/10.1007/s10579-008-9076-6',
                foundational: true
            },
            {
                id: 'hsu2021hubert',
                title: 'HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units',
                authors: 'Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman',
                year: 2021,
                venue: 'IEEE/ACM Transactions on Audio, Speech, and Language Processing',
                category: 'ssl',
                doi: '10.1109/TASLP.2021.3122291',
                url: 'https://doi.org/10.1109/TASLP.2021.3122291',
                foundational: true
            },
            {
                id: 'baevski2020wav2vec2',
                title: 'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations',
                authors: 'Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael',
                year: 2020,
                venue: 'NeurIPS 2020',
                category: 'ssl',
                arxiv: '2006.11477',
                doi: '10.48550/arXiv.2006.11477',
                url: 'https://arxiv.org/abs/2006.11477',
                foundational: true
            },
            {
                id: 'radford2023whisper',
                title: 'Robust Speech Recognition via Large-Scale Weak Supervision',
                authors: 'Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya',
                year: 2023,
                venue: 'arXiv preprint',
                category: 'technical',
                arxiv: '2212.04356',
                doi: '10.48550/arXiv.2212.04356',
                url: 'https://arxiv.org/abs/2212.04356',
                foundational: true
            },
            {
                id: 'vaswani2017attention',
                title: 'Attention is All You Need',
                authors: 'Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Åukasz and Polosukhin, Illia',
                year: 2017,
                venue: 'NeurIPS 2017',
                category: 'technical',
                arxiv: '1706.03762',
                doi: '10.48550/arXiv.1706.03762',
                url: 'https://arxiv.org/abs/1706.03762',
                foundational: true
            },
            {
                id: 'ho2020ddpm',
                title: 'Denoising Diffusion Probabilistic Models',
                authors: 'Ho, Jonathan and Jain, Ajay and Abbeel, Pieter',
                year: 2020,
                venue: 'NeurIPS 2020',
                category: 'technical',
                arxiv: '2006.11239',
                doi: '10.48550/arXiv.2006.11239',
                url: 'https://arxiv.org/abs/2006.11239',
                foundational: true
            },
            {
                id: 'peebles2023dit',
                title: 'Scalable Diffusion Models with Transformers',
                authors: 'Peebles, William and Xie, Saining',
                year: 2023,
                venue: 'arXiv preprint',
                category: 'technical',
                arxiv: '2212.09748',
                doi: '10.48550/arXiv.2212.09748',
                url: 'https://arxiv.org/abs/2212.09748',
                foundational: true
            }
        ];
        
        let filteredPapers = [...papers];
        let currentYearFilter = 'all';
        let currentCategoryFilter = 'all';
        let currentSort = 'year-desc';
        
        function updateStats() {
            document.getElementById('totalPapers').textContent = papers.length;
            document.getElementById('year2023').textContent = papers.filter(p => p.year === 2023).length;
            document.getElementById('year2024').textContent = papers.filter(p => p.year === 2024).length;
            document.getElementById('year2025').textContent = papers.filter(p => p.year === 2025).length;
            document.getElementById('foundational').textContent = papers.filter(p => p.foundational).length;
        }
        
        function getCategoryDisplay(category) {
            const categoryMap = {
                'codec': 'Neural Codec LM',
                'flow': 'Flow/Diffusion',
                'disentangle': 'Disentanglement',
                'controllable': 'Controllable',
                'multimodal': 'Multimodal',
                'watermark': 'Watermarking',
                'dataset': 'Dataset',
                'ssl': 'Self-Supervised Learning',
                'technical': 'Technical/Foundation'
            };
            return categoryMap[category] || category;
        }
        
        function renderPapers() {
            const container = document.getElementById('papersContainer');
            const noResults = document.getElementById('noResults');
            
            if (filteredPapers.length === 0) {
                container.innerHTML = '';
                noResults.classList.remove('hidden');
                return;
            }
            
            noResults.classList.add('hidden');
            
            // Group by year
            const groupedByYear = {};
            filteredPapers.forEach(paper => {
                const yearKey = paper.foundational ? 'Foundational' : paper.year;
                if (!groupedByYear[yearKey]) {
                    groupedByYear[yearKey] = [];
                }
                groupedByYear[yearKey].push(paper);
            });
            
            // Sort years
            const sortedYears = Object.keys(groupedByYear).sort((a, b) => {
                if (a === 'Foundational') return 1;
                if (b === 'Foundational') return -1;
                return b - a;
            });
            
            container.innerHTML = '';
            
            sortedYears.forEach(year => {
                const yearDiv = document.createElement('div');
                yearDiv.className = 'year-group';
                
                const yearHeader = document.createElement('div');
                yearHeader.className = 'year-header';
                yearHeader.innerHTML = `
                    <span>${year}</span>
                    <span class="year-count">${groupedByYear[year].length} papers</span>
                `;
                yearDiv.appendChild(yearHeader);
                
                const papersContainer = document.createElement('div');
                papersContainer.className = 'papers-container';
                
                groupedByYear[year].forEach(paper => {
                    const card = document.createElement('div');
                    card.className = 'paper-card';
                    
                    const firstAuthor = paper.authors.split(' and ')[0].split(',')[0];
                    
                    card.innerHTML = `
                        <div class="paper-header">
                            <div class="paper-title">${paper.title}</div>
                            <div class="paper-year">${paper.year}</div>
                        </div>
                        <div class="paper-meta">
                            <span class="paper-category category-${paper.category}">${getCategoryDisplay(paper.category)}</span>
                            <span class="paper-venue">üìç ${paper.venue}</span>
                        </div>
                        <div class="paper-authors">
                            <strong>Authors:</strong> ${paper.authors}
                        </div>
                        <div class="paper-links">
                            ${paper.url ? `<a href="${paper.url}" target="_blank" class="paper-link">üìÑ Paper</a>` : ''}
                            ${paper.arxiv ? `<a href="https://arxiv.org/abs/${paper.arxiv}" target="_blank" class="paper-link">arXiv</a>` : ''}
                            ${paper.doi ? `<a href="https://doi.org/${paper.doi}" target="_blank" class="paper-link doi">DOI</a>` : ''}
                        </div>
                    `;
                    
                    papersContainer.appendChild(card);
                });
                
                yearDiv.appendChild(papersContainer);
                container.appendChild(yearDiv);
            });
            
            updateResultsInfo();
        }
        
        function updateResultsInfo() {
            const info = document.getElementById('resultsInfo');
            let text = `Showing ${filteredPapers.length} of ${papers.length} papers`;
            
            if (currentYearFilter !== 'all') {
                text += ` ‚Ä¢ Filtered by year: ${currentYearFilter}`;
            }
            if (currentCategoryFilter !== 'all') {
                text += ` ‚Ä¢ Category: ${getCategoryDisplay(currentCategoryFilter)}`;
            }
            
            info.textContent = text;
        }
        
        function applyFilters() {
            filteredPapers = papers.filter(paper => {
                // Year filter
                if (currentYearFilter !== 'all') {
                    if (currentYearFilter === 'foundational' && !paper.foundational) return false;
                    if (currentYearFilter !== 'foundational' && paper.year !== parseInt(currentYearFilter)) return false;
                }
                
                // Category filter
                if (currentCategoryFilter !== 'all' && paper.category !== currentCategoryFilter) {
                    return false;
                }
                
                // Search filter
                const searchTerm = document.getElementById('searchBox').value.toLowerCase();
                if (searchTerm) {
                    const searchableText = `${paper.title} ${paper.authors} ${paper.venue}`.toLowerCase();
                    if (!searchableText.includes(searchTerm)) return false;
                }
                
                return true;
            });
            
            sortPapers();
        }
        
        function filterYear(year) {
            currentYearFilter = year;
            document.querySelectorAll('#yearFilters .filter-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            event.target.classList.add('active');
            applyFilters();
        }
        
        function filterCategory(category) {
            currentCategoryFilter = category;
            document.querySelectorAll('#categoryFilters .filter-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            event.target.classList.add('active');
            applyFilters();
        }
        
        function sortPapers() {
            const sortBy = document.getElementById('sortSelect').value;
            
            filteredPapers.sort((a, b) => {
                switch(sortBy) {
                    case 'year-desc':
                        return b.year - a.year;
                    case 'year-asc':
                        return a.year - b.year;
                    case 'title':
                        return a.title.localeCompare(b.title);
                    case 'authors':
                        return a.authors.localeCompare(b.authors);
                    default:
                        return 0;
                }
            });
            
            renderPapers();
        }
        
        function exportBibTeX() {
            let bibtex = '% Voice Cloning Bibliography (2023-2025)\n% Generated from Interactive Bibliography Browser\n\n';
            
            filteredPapers.forEach(paper => {
                bibtex += `@article{${paper.id},\n`;
                bibtex += `  title={${paper.title}},\n`;
                bibtex += `  author={${paper.authors}},\n`;
                bibtex += `  year={${paper.year}},\n`;
                bibtex += `  venue={${paper.venue}},\n`;
                if (paper.doi) bibtex += `  doi={${paper.doi}},\n`;
                if (paper.url) bibtex += `  url={${paper.url}},\n`;
                bibtex += `}\n\n`;
            });
            
            downloadFile(bibtex, 'voice_cloning_bibliography.bib', 'text/plain');
        }
        
        function exportJSON() {
            const json = JSON.stringify(filteredPapers, null, 2);
            downloadFile(json, 'voice_cloning_bibliography.json', 'application/json');
        }
        
        function downloadFile(content, filename, type) {
            const blob = new Blob([content], { type: type });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = filename;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }
        
        // Event listeners
        document.getElementById('searchBox').addEventListener('input', applyFilters);
        document.getElementById('sortSelect').addEventListener('change', sortPapers);
        
        // Initial render
        updateStats();
        applyFilters();
    </script>
</body>
</html>
