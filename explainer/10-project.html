<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 10: Hands-On Project | NLP Course</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --bg: #0a0a0f;
            --surface: #12121a;
            --card: #1a1a2e;
            --border: #2a2a3e;
            --accent: #6c63ff;
            --accent2: #00d2ff;
            --accent3: #ff6b6b;
            --text: #e0e0e8;
            --muted: #8888a0;
            --font: 'Inter', system-ui, -apple-system, sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            background: var(--bg);
            color: var(--text);
            font-family: var(--font);
            line-height: 1.6;
            overflow-x: hidden;
        }

        /* Navigation Bar */
        .nav-bar {
            position: sticky;
            top: 0;
            z-index: 1000;
            background: rgba(10, 10, 15, 0.95);
            backdrop-filter: blur(10px);
            border-bottom: 1px solid var(--border);
            padding: 1rem 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-bar a {
            color: var(--accent2);
            text-decoration: none;
            font-weight: 500;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            transition: color 0.3s;
        }

        .nav-bar a:hover {
            color: var(--accent);
        }

        .nav-title {
            color: var(--muted);
            font-size: 0.9rem;
            font-weight: 400;
        }

        /* Main Container */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        /* Hero Section */
        .hero {
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center;
            gap: 2rem;
            padding: 4rem 2rem;
            background: linear-gradient(135deg, rgba(108, 99, 255, 0.1) 0%, rgba(0, 210, 255, 0.05) 100%);
            border-radius: 20px;
            margin: 2rem 0;
            position: relative;
            overflow: hidden;
        }

        .hero::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -10%;
            width: 500px;
            height: 500px;
            background: radial-gradient(circle, rgba(108, 99, 255, 0.15) 0%, transparent 70%);
            border-radius: 50%;
            pointer-events: none;
        }

        .hero-content {
            position: relative;
            z-index: 1;
        }

        .module-label {
            display: inline-block;
            background: rgba(108, 99, 255, 0.2);
            color: var(--accent2);
            padding: 0.5rem 1.5rem;
            border-radius: 50px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 1rem;
        }

        .hero h1 {
            font-size: 3.5rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, #6c63ff 0%, #00d2ff 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .hero-subtitle {
            font-size: 1.3rem;
            color: var(--muted);
            max-width: 600px;
            margin-bottom: 1rem;
        }

        .hero-meta {
            display: flex;
            justify-content: center;
            gap: 2rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--muted);
        }

        .meta-item strong {
            color: var(--accent2);
        }

        .celebration {
            font-size: 3rem;
            animation: bounce 2s infinite;
            margin-top: 1rem;
        }

        @keyframes bounce {
            0%, 100% { transform: translateY(0); }
            50% { transform: translateY(-20px); }
        }

        /* Section */
        .section {
            margin: 4rem 0;
            padding: 3rem 2rem;
            background: linear-gradient(135deg, rgba(26, 26, 46, 0.8) 0%, rgba(18, 18, 26, 0.8) 100%);
            border: 1px solid var(--border);
            border-radius: 15px;
            opacity: 0;
            animation: fadeIn 0.8s ease-out forwards;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .section:nth-child(even) {
            animation-delay: 0.1s;
        }

        .section:nth-child(3n) {
            animation-delay: 0.2s;
        }

        .section h2 {
            font-size: 2.2rem;
            margin-bottom: 1.5rem;
            color: var(--accent2);
        }

        .section h3 {
            font-size: 1.5rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--accent);
        }

        /* Code Block */
        .code-block {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            position: relative;
        }

        .code-block pre {
            margin: 0;
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 0.85rem;
            line-height: 1.5;
            color: var(--text);
        }

        .code-line {
            display: block;
            padding: 0.2rem 0;
            cursor: pointer;
            transition: background 0.2s;
            border-left: 3px solid transparent;
        }

        .code-line:hover {
            background: rgba(108, 99, 255, 0.1);
            border-left-color: var(--accent);
        }

        .line-number {
            color: var(--muted);
            margin-right: 1.5rem;
            user-select: none;
            min-width: 40px;
            display: inline-block;
        }

        .code-copy {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: var(--accent);
            color: var(--bg);
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            cursor: pointer;
            font-size: 0.8rem;
            font-weight: 600;
            transition: background 0.2s;
        }

        .code-copy:hover {
            background: var(--accent2);
        }

        /* Walkthrough */
        .walkthrough {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 2rem 0;
            align-items: start;
        }

        @media (max-width: 900px) {
            .walkthrough {
                grid-template-columns: 1fr;
            }
        }

        .walkthrough-code {
            max-height: 500px;
            overflow-y: auto;
        }

        .walkthrough-explanation {
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem;
            min-height: 300px;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }

        .walkthrough-explanation h4 {
            color: var(--accent2);
            margin-bottom: 1rem;
            font-size: 1.2rem;
        }

        .walkthrough-explanation p {
            color: var(--muted);
            line-height: 1.7;
        }

        .code-section {
            padding: 1rem;
            margin: 0.5rem 0;
            border-radius: 5px;
            cursor: pointer;
            transition: all 0.3s;
            border-left: 3px solid transparent;
        }

        .code-section:hover {
            background: rgba(108, 99, 255, 0.15);
            border-left-color: var(--accent);
        }

        .code-section.active {
            background: rgba(108, 99, 255, 0.25);
            border-left-color: var(--accent2);
        }

        /* Input Box */
        .input-group {
            display: flex;
            gap: 1rem;
            margin: 1.5rem 0;
            flex-wrap: wrap;
        }

        input[type="text"], textarea {
            flex: 1;
            min-width: 250px;
            background: var(--surface);
            border: 1px solid var(--border);
            color: var(--text);
            padding: 0.75rem 1rem;
            border-radius: 8px;
            font-family: var(--font);
            font-size: 1rem;
            transition: border-color 0.3s;
        }

        input[type="text"]:focus, textarea:focus {
            outline: none;
            border-color: var(--accent);
        }

        textarea {
            min-height: 100px;
            resize: vertical;
        }

        button {
            background: linear-gradient(135deg, var(--accent) 0%, var(--accent2) 100%);
            color: var(--bg);
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 8px;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            font-family: var(--font);
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(108, 99, 255, 0.3);
        }

        button:active {
            transform: translateY(0);
        }

        /* Demo Section */
        .demo-result {
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            min-height: 150px;
        }

        .result-label {
            color: var(--muted);
            font-size: 0.9rem;
            margin-bottom: 0.5rem;
        }

        .result-main {
            font-size: 1.5rem;
            font-weight: 700;
            margin-bottom: 1rem;
        }

        .result-positive {
            color: var(--accent2);
        }

        .result-negative {
            color: var(--accent3);
        }

        .confidence-bar {
            height: 30px;
            background: var(--surface);
            border-radius: 8px;
            overflow: hidden;
            margin-top: 1rem;
        }

        .confidence-fill {
            height: 100%;
            background: linear-gradient(90deg, var(--accent) 0%, var(--accent2) 100%);
            display: flex;
            align-items: center;
            justify-content: flex-end;
            padding-right: 1rem;
            color: var(--bg);
            font-weight: 600;
            font-size: 0.9rem;
            transition: width 0.3s ease-out;
        }

        /* Pipeline Visualization */
        .pipeline {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 2rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .pipeline-step {
            flex: 1;
            min-width: 120px;
            text-align: center;
            padding: 1.5rem;
            background: var(--card);
            border: 2px solid var(--border);
            border-radius: 10px;
            transition: all 0.3s;
        }

        .pipeline-step:hover {
            border-color: var(--accent);
            background: rgba(108, 99, 255, 0.1);
        }

        .pipeline-step-label {
            font-size: 0.85rem;
            color: var(--muted);
            text-transform: uppercase;
            margin-bottom: 0.5rem;
        }

        .pipeline-step-icon {
            font-size: 2rem;
            margin: 0.5rem 0;
        }

        .pipeline-arrow {
            flex: 0 0 30px;
            height: 3px;
            background: linear-gradient(90deg, var(--accent) 0%, transparent 100%);
            position: relative;
        }

        .pipeline-arrow::after {
            content: '‚Üí';
            position: absolute;
            right: -10px;
            top: -10px;
            color: var(--accent);
            font-weight: bold;
        }

        /* Card Grid */
        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin: 1.5rem 0;
        }

        .card {
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem;
            cursor: pointer;
            transition: all 0.3s;
        }

        .card:hover {
            border-color: var(--accent);
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(108, 99, 255, 0.2);
        }

        .card h4 {
            color: var(--accent2);
            margin-bottom: 1rem;
        }

        .card p {
            color: var(--muted);
            font-size: 0.95rem;
        }

        .card.expandable {
            max-height: 150px;
            overflow: hidden;
            position: relative;
        }

        .card.expandable.expanded {
            max-height: 1000px;
        }

        .card.expandable::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            height: 40px;
            background: linear-gradient(transparent, var(--card));
            pointer-events: none;
        }

        .card.expandable.expanded::after {
            display: none;
        }

        /* Timeline */
        .timeline {
            position: relative;
            padding: 2rem 0;
        }

        .timeline::before {
            content: '';
            position: absolute;
            left: 50%;
            transform: translateX(-50%);
            width: 3px;
            height: 100%;
            background: linear-gradient(180deg, var(--accent) 0%, var(--accent2) 100%);
            border-radius: 2px;
        }

        @media (max-width: 768px) {
            .timeline::before {
                left: 30px;
            }
        }

        .timeline-item {
            margin-bottom: 2rem;
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            align-items: center;
        }

        @media (max-width: 768px) {
            .timeline-item {
                grid-template-columns: 60px 1fr;
                gap: 1rem;
            }
        }

        .timeline-item:nth-child(even) {
            direction: rtl;
        }

        .timeline-item:nth-child(even) > * {
            direction: ltr;
        }

        @media (max-width: 768px) {
            .timeline-item:nth-child(even) {
                direction: ltr;
            }
        }

        .timeline-dot {
            width: 20px;
            height: 20px;
            background: var(--accent2);
            border: 3px solid var(--bg);
            border-radius: 50%;
            position: absolute;
            left: 50%;
            transform: translateX(-50%);
            z-index: 1;
        }

        @media (max-width: 768px) {
            .timeline-dot {
                left: 30px;
            }
        }

        .timeline-content {
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem;
            position: relative;
        }

        .timeline-number {
            display: inline-block;
            background: var(--accent);
            color: var(--bg);
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            margin-bottom: 1rem;
        }

        .timeline-content h4 {
            color: var(--accent2);
            margin-bottom: 0.5rem;
        }

        .timeline-content p {
            color: var(--muted);
            font-size: 0.95rem;
        }

        /* Certificate */
        .certificate {
            background: linear-gradient(135deg, rgba(108, 99, 255, 0.1) 0%, rgba(0, 210, 255, 0.1) 100%);
            border: 2px solid var(--accent);
            border-radius: 15px;
            padding: 3rem;
            text-align: center;
            margin: 2rem 0;
            position: relative;
            overflow: hidden;
        }

        .certificate::before {
            content: '‚úì';
            position: absolute;
            top: -50px;
            right: -50px;
            font-size: 200px;
            color: rgba(108, 99, 255, 0.1);
            font-weight: bold;
        }

        .certificate-title {
            color: var(--accent2);
            font-size: 1.3rem;
            text-transform: uppercase;
            letter-spacing: 2px;
            margin-bottom: 1rem;
        }

        .certificate-subtitle {
            color: var(--muted);
            font-size: 1.1rem;
            margin-bottom: 2rem;
        }

        .badge-row {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            margin: 2rem 0;
        }

        .badge {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0.5rem;
        }

        .badge-icon {
            font-size: 3rem;
        }

        .badge-text {
            color: var(--muted);
            font-size: 0.9rem;
        }

        /* Footer */
        .nav-footer {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 1px solid var(--border);
            flex-wrap: wrap;
            gap: 1rem;
        }

        .nav-footer a {
            color: var(--accent2);
            text-decoration: none;
            font-weight: 500;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            transition: color 0.3s;
        }

        .nav-footer a:hover {
            color: var(--accent);
        }

        /* Tabs */
        .tabs {
            display: flex;
            gap: 1rem;
            margin-bottom: 1.5rem;
            border-bottom: 1px solid var(--border);
            flex-wrap: wrap;
        }

        .tab-button {
            background: none;
            color: var(--muted);
            border: none;
            padding: 0.75rem 1.5rem;
            cursor: pointer;
            transition: all 0.3s;
            border-bottom: 2px solid transparent;
            font-weight: 500;
        }

        .tab-button:hover {
            color: var(--accent2);
        }

        .tab-button.active {
            color: var(--accent2);
            border-bottom-color: var(--accent2);
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        /* Sentiment Examples */
        .sentiment-examples {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }

        .sentiment-item {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
            cursor: pointer;
            transition: all 0.3s;
        }

        .sentiment-item:hover {
            border-color: var(--accent);
            background: rgba(108, 99, 255, 0.05);
        }

        .sentiment-item-text {
            color: var(--text);
            font-size: 0.95rem;
            margin-bottom: 0.75rem;
            font-style: italic;
        }

        .sentiment-item-result {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .sentiment-badge {
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
        }

        .sentiment-positive {
            background: rgba(0, 210, 255, 0.2);
            color: var(--accent2);
        }

        .sentiment-negative {
            background: rgba(255, 107, 107, 0.2);
            color: var(--accent3);
        }

        .sentiment-confidence {
            color: var(--muted);
            font-size: 0.85rem;
        }

        /* Loss Curve */
        .loss-curve-container {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 2rem;
            margin: 1.5rem 0;
            text-align: center;
        }

        /* BLEU Calculator */
        .bleu-calculator {
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 2rem;
            margin: 1.5rem 0;
        }

        .bleu-result {
            background: var(--surface);
            border-left: 4px solid var(--accent2);
            border-radius: 5px;
            padding: 1rem;
            margin-top: 1rem;
        }

        .bleu-score {
            font-size: 2rem;
            font-weight: 700;
            color: var(--accent2);
        }

        .bleu-info {
            color: var(--muted);
            font-size: 0.9rem;
            margin-top: 0.5rem;
        }

        /* Resources Grid */
        .resources-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1.5rem;
            margin: 1.5rem 0;
        }

        .resource-card {
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem;
            transition: all 0.3s;
        }

        .resource-card:hover {
            border-color: var(--accent);
            box-shadow: 0 10px 30px rgba(108, 99, 255, 0.15);
        }

        .resource-icon {
            font-size: 2rem;
            margin-bottom: 1rem;
        }

        .resource-card h4 {
            color: var(--accent2);
            margin-bottom: 0.5rem;
        }

        .resource-card p {
            color: var(--muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }

        .resource-card a {
            color: var(--accent);
            text-decoration: none;
            font-weight: 500;
            font-size: 0.9rem;
        }

        .resource-card a:hover {
            color: var(--accent2);
        }

        /* Data Examples */
        .data-pair {
            background: var(--surface);
            border-left: 4px solid var(--accent);
            border-radius: 5px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .data-pair-lang {
            color: var(--muted);
            font-size: 0.85rem;
            text-transform: uppercase;
            margin-bottom: 0.25rem;
        }

        .data-pair-text {
            color: var(--text);
            font-style: italic;
            line-height: 1.6;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .hero h1 {
                font-size: 2.2rem;
            }

            .hero-subtitle {
                font-size: 1.1rem;
            }

            .section {
                padding: 1.5rem 1rem;
            }

            .walkthrough {
                grid-template-columns: 1fr;
            }

            .pipeline {
                flex-direction: column;
                gap: 1rem;
            }

            .pipeline-arrow {
                transform: rotate(90deg);
                width: 3px;
                height: 30px;
                flex: 0 0 30px;
            }

            .timeline::before {
                left: 30px;
            }

            .certificate {
                padding: 1.5rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="nav-bar">
        <a href="index.html">‚Üê Back to Course</a>
        <span class="nav-title">Module 10: Hands-On Project</span>
    </nav>

    <div class="container">
        <!-- Hero Section -->
        <section class="hero">
            <div class="hero-content">
                <span class="module-label">Module 10 of 10</span>
                <h1>Hands-On Project</h1>
                <p class="hero-subtitle">From Theory to Working Code</p>
                <div class="hero-meta">
                    <div class="meta-item">
                        <strong>Duration:</strong> ~40 min
                    </div>
                    <div class="meta-item">
                        <strong>Level:</strong> Intermediate
                    </div>
                    <div class="meta-item">
                        <strong>Type:</strong> Code + Implementation
                    </div>
                </div>
                <div class="celebration">üéâ</div>
            </div>
        </section>

        <!-- Project Overview -->
        <section class="section">
            <h2>Your Mission</h2>
            <p>You've learned NLP theory across 9 modules. Now it's time to build. In this final module, you'll implement two real-world projects:</p>
            <div class="card-grid">
                <div class="card">
                    <h4>üöÄ Project A: Text Classifier</h4>
                    <p>Build a sentiment classifier in under 50 lines using HuggingFace and fine-tune a pre-trained model on real data.</p>
                </div>
                <div class="card">
                    <h4>üèõÔ∏è Project B: Ancient Translation</h4>
                    <p>Design a complete architecture for translating ancient languages‚Äîfrom data collection to deployment, with production-ready code.</p>
                </div>
            </div>
            <p style="margin-top: 1.5rem; color: var(--muted); font-size: 0.95rem;">Both projects are deeply interconnected. Project A teaches you core patterns that apply directly to Project B.</p>
        </section>

        <!-- Project A: Text Classifier -->
        <section class="section">
            <h2>Project A: 50-Line Text Classifier</h2>
            <p>Build a sentiment classifier using HuggingFace Transformers. This code is real, complete, and runnable.</p>

            <h3>The Complete Code</h3>
            <div class="code-block">
                <button class="code-copy" onclick="copyCode('classifier-code')">Copy Code</button>
                <pre id="classifier-code"><span class="code-line"><span class="line-number">1</span>from datasets import load_dataset
<span class="code-line"><span class="line-number">2</span>from transformers import AutoTokenizer, AutoModelForSequenceClassification
<span class="code-line"><span class="line-number">3</span>from transformers import TrainingArguments, Trainer
<span class="code-line"><span class="line-number">4</span>import numpy as np
<span class="code-line"><span class="line-number">5</span>
<span class="code-line"><span class="line-number">6</span># Load pre-trained model & tokenizer
<span class="code-line"><span class="line-number">7</span>model_name = "distilbert-base-uncased"
<span class="code-line"><span class="line-number">8</span>tokenizer = AutoTokenizer.from_pretrained(model_name)
<span class="code-line"><span class="line-number">9</span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="code-line"><span class="line-number">10</span>    model_name, num_labels=2
<span class="code-line"><span class="line-number">11</span>)
<span class="code-line"><span class="line-number">12</span>
<span class="code-line"><span class="line-number">13</span># Load & prepare dataset
<span class="code-line"><span class="line-number">14</span>dataset = load_dataset("imdb")
<span class="code-line"><span class="line-number">14</span>train_set = dataset["train"].select(range(1000))  # 1000 samples for speed
<span class="code-line"><span class="line-number">15</span>test_set = dataset["test"].select(range(200))
<span class="code-line"><span class="line-number">16</span>
<span class="code-line"><span class="line-number">17</span># Tokenize
<span class="code-line"><span class="line-number">18</span>def tokenize(batch):
<span class="code-line"><span class="line-number">19</span>    return tokenizer(batch["text"], padding="max_length", truncation=True)
<span class="code-line"><span class="line-number">20</span>
<span class="code-line"><span class="line-number">21</span>train_set = train_set.map(tokenize, batched=True)
<span class="code-line"><span class="line-number">22</span>test_set = test_set.map(tokenize, batched=True)
<span class="code-line"><span class="line-number">23</span>train_set.set_format("torch", columns=["input_ids", "attention_mask", "label"])
<span class="code-line"><span class="line-number">24</span>test_set.set_format("torch", columns=["input_ids", "attention_mask", "label"])
<span class="code-line"><span class="line-number">25</span>
<span class="code-line"><span class="line-number">26</span># Training setup
<span class="code-line"><span class="line-number">27</span>training_args = TrainingArguments(
<span class="code-line"><span class="line-number">28</span>    output_dir="./sentiment_model",
<span class="code-line"><span class="line-number">29</span>    num_train_epochs=2,
<span class="code-line"><span class="line-number">30</span>    per_device_train_batch_size=8,
<span class="code-line"><span class="line-number">31</span>    per_device_eval_batch_size=16,
<span class="code-line"><span class="line-number">32</span>    warmup_steps=100,
<span class="code-line"><span class="line-number">33</span>    weight_decay=0.01,
<span class="code-line"><span class="line-number">34</span>    logging_steps=50,
<span class="code-line"><span class="line-number">35</span>    eval_strategy="epoch"
<span class="code-line"><span class="line-number">36</span>)
<span class="code-line"><span class="line-number">37</span>
<span class="code-line"><span class="line-number">38</span>trainer = Trainer(
<span class="code-line"><span class="line-number">39</span>    model=model, args=training_args, train_dataset=train_set,
<span class="code-line"><span class="line-number">40</span>    eval_dataset=test_set
<span class="code-line"><span class="line-number">41</span>)
<span class="code-line"><span class="line-number">42</span>
<span class="code-line"><span class="line-number">43</span># Train & evaluate
<span class="code-line"><span class="line-number">44</span>trainer.train()
<span class="code-line"><span class="line-number">45</span>eval_results = trainer.evaluate()
<span class="code-line"><span class="line-number">46</span>print(f"Accuracy: {eval_results['eval_accuracy']:.4f}")
<span class="code-line"><span class="line-number">47</span>
<span class="code-line"><span class="line-number">48</span># Predict
<span class="code-line"><span class="line-number">49</span>text = "This movie is absolutely fantastic!"
<span class="code-line"><span class="line-number">50</span>inputs = tokenizer(text, return_tensors="pt")
<span class="code-line"><span class="line-number">51</span>outputs = model(**inputs)
<span class="code-line"><span class="line-number">52</span>prediction = np.argmax(outputs.logits.detach().numpy(), axis=1)
<span class="code-line"><span class="line-number">53</span>print(f"Sentiment: {'Positive' if prediction[0] == 1 else 'Negative'}")</span></pre>
            </div>

            <h3>Code Walkthrough</h3>
            <div class="walkthrough">
                <div class="walkthrough-code">
                    <div class="code-section" onclick="showExplanation(this, 'imports')">
                        <strong>Lines 1-4: Imports</strong>
                        <p style="font-size: 0.85rem; color: var(--muted); margin-top: 0.5rem;">Load required libraries</p>
                    </div>
                    <div class="code-section" onclick="showExplanation(this, 'model')">
                        <strong>Lines 7-11: Model Loading</strong>
                        <p style="font-size: 0.85rem; color: var(--muted); margin-top: 0.5rem;">Download DistilBERT weights</p>
                    </div>
                    <div class="code-section" onclick="showExplanation(this, 'data')">
                        <strong>Lines 14-15: Dataset</strong>
                        <p style="font-size: 0.85rem; color: var(--muted); margin-top: 0.5rem;">Load IMDB dataset</p>
                    </div>
                    <div class="code-section" onclick="showExplanation(this, 'tokenize')">
                        <strong>Lines 18-24: Tokenization</strong>
                        <p style="font-size: 0.85rem; color: var(--muted); margin-top: 0.5rem;">Convert text to tokens</p>
                    </div>
                    <div class="code-section" onclick="showExplanation(this, 'training')">
                        <strong>Lines 27-41: Training</strong>
                        <p style="font-size: 0.85rem; color: var(--muted); margin-top: 0.5rem;">Set up & run trainer</p>
                    </div>
                    <div class="code-section" onclick="showExplanation(this, 'eval')">
                        <strong>Lines 44-53: Inference</strong>
                        <p style="font-size: 0.85rem; color: var(--muted); margin-top: 0.5rem;">Evaluate & predict</p>
                    </div>
                </div>
                <div class="walkthrough-explanation" id="explanation-panel">
                    <p style="text-align: center; color: var(--muted);">Click a code section ‚Üí</p>
                </div>
            </div>

            <h3>Key Takeaways</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>Pre-training is Power</h4>
                    <p>DistilBERT came pre-trained on 16GB of English text. We just fine-tune it‚Äîsaving compute and getting better results.</p>
                </div>
                <div class="card">
                    <h4>Tokenization is Critical</h4>
                    <p>The tokenizer breaks text into subwords learned during pre-training. Mismatch = bad performance.</p>
                </div>
                <div class="card">
                    <h4>HuggingFace Trainer is MVP</h4>
                    <p>Handles mixed precision, gradient accumulation, and checkpointing. Use it instead of raw PyTorch loops.</p>
                </div>
                <div class="card">
                    <h4>50 Lines = Production Ready</h4>
                    <p>This isn't a toy. With logging, validation, and best practices, this pattern scales to 100M+ parameter models.</p>
                </div>
            </div>
        </section>

        <!-- Interactive Classifier Demo -->
        <section class="section">
            <h2>Interactive Classifier Demo</h2>
            <p>Test the classifier with your own movie reviews:</p>

            <div class="input-group">
                <textarea id="review-input" placeholder="Enter a movie review here..."></textarea>
                <button onclick="classifyReview()">Classify</button>
            </div>

            <div id="demo-result" style="display: none;">
                <div class="demo-result">
                    <div class="result-label">Prediction</div>
                    <div class="result-main" id="sentiment-label"></div>
                    <div class="result-label">Confidence</div>
                    <div class="confidence-bar">
                        <div class="confidence-fill" id="confidence-fill" style="width: 0%;"></div>
                    </div>
                </div>
            </div>

            <h3>Pre-computed Examples</h3>
            <div class="sentiment-examples" id="sentiment-examples"></div>

            <h3>Classification Pipeline</h3>
            <div class="pipeline">
                <div class="pipeline-step">
                    <div class="pipeline-step-label">Input</div>
                    <div class="pipeline-step-icon">üìù</div>
                    <div style="font-size: 0.9rem; color: var(--text);">Raw text</div>
                </div>
                <div class="pipeline-arrow"></div>
                <div class="pipeline-step">
                    <div class="pipeline-step-label">Tokenize</div>
                    <div class="pipeline-step-icon">üî§</div>
                    <div style="font-size: 0.9rem; color: var(--text);">Token IDs</div>
                </div>
                <div class="pipeline-arrow"></div>
                <div class="pipeline-step">
                    <div class="pipeline-step-label">Encode</div>
                    <div class="pipeline-step-icon">üìä</div>
                    <div style="font-size: 0.9rem; color: var(--text);">Embeddings</div>
                </div>
                <div class="pipeline-arrow"></div>
                <div class="pipeline-step">
                    <div class="pipeline-step-label">Forward</div>
                    <div class="pipeline-step-icon">üß†</div>
                    <div style="font-size: 0.9rem; color: var(--text);">Logits</div>
                </div>
                <div class="pipeline-arrow"></div>
                <div class="pipeline-step">
                    <div class="pipeline-step-label">Softmax</div>
                    <div class="pipeline-step-icon">üìà</div>
                    <div style="font-size: 0.9rem; color: var(--text);">Probabilities</div>
                </div>
            </div>
        </section>

        <!-- Project B: Ancient Language Translation -->
        <section class="section">
            <h2>Project B: Ancient Language Translation Architecture</h2>
            <p>Translating ancient languages requires a different approach than modern text. Here's the complete architecture.</p>

            <h3>Full System Architecture</h3>
            <svg viewBox="0 0 1000 400" style="width: 100%; max-height: 400px; margin: 2rem 0;">
                <!-- Background -->
                <rect width="1000" height="400" fill="none" stroke="rgba(108, 99, 255, 0.1)" stroke-width="1"/>

                <!-- Title -->
                <text x="500" y="30" text-anchor="middle" fill="#00d2ff" font-size="18" font-weight="bold">Ancient Language Translation Pipeline</text>

                <!-- Step boxes -->
                <g id="steps">
                    <!-- Data Collection -->
                    <rect x="20" y="80" width="120" height="100" fill="rgba(108, 99, 255, 0.15)" stroke="#6c63ff" stroke-width="2" rx="8"/>
                    <text x="80" y="115" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="bold">Data</text>
                    <text x="80" y="133" text-anchor="middle" fill="#8888a0" font-size="11">Collection</text>
                    <text x="80" y="153" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Corpora</text>
                    <text x="80" y="168" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Alignment</text>

                    <!-- Arrow 1 -->
                    <line x1="140" y1="130" x2="180" y2="130" stroke="#00d2ff" stroke-width="2" marker-end="url(#arrowhead)"/>

                    <!-- Preprocessing -->
                    <rect x="180" y="80" width="120" height="100" fill="rgba(0, 210, 255, 0.15)" stroke="#00d2ff" stroke-width="2" rx="8"/>
                    <text x="240" y="115" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="bold">Preprocess</text>
                    <text x="240" y="133" text-anchor="middle" fill="#8888a0" font-size="11">Cleaning</text>
                    <text x="240" y="153" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Normalization</text>
                    <text x="240" y="168" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Alignment</text>

                    <!-- Arrow 2 -->
                    <line x1="300" y1="130" x2="340" y2="130" stroke="#00d2ff" stroke-width="2" marker-end="url(#arrowhead)"/>

                    <!-- Tokenizer -->
                    <rect x="340" y="80" width="120" height="100" fill="rgba(108, 99, 255, 0.15)" stroke="#6c63ff" stroke-width="2" rx="8"/>
                    <text x="400" y="115" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="bold">Tokenizer</text>
                    <text x="400" y="133" text-anchor="middle" fill="#8888a0" font-size="11">Train BPE</text>
                    <text x="400" y="153" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Vocab</text>
                    <text x="400" y="168" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Building</text>

                    <!-- Arrow 3 -->
                    <line x1="460" y1="130" x2="500" y2="130" stroke="#00d2ff" stroke-width="2" marker-end="url(#arrowhead)"/>

                    <!-- Model Selection -->
                    <rect x="500" y="80" width="120" height="100" fill="rgba(255, 107, 107, 0.15)" stroke="#ff6b6b" stroke-width="2" rx="8"/>
                    <text x="560" y="115" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="bold">Model</text>
                    <text x="560" y="133" text-anchor="middle" fill="#8888a0" font-size="11">Choose Arch</text>
                    <text x="560" y="153" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">mT5, mBART</text>
                    <text x="560" y="168" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">or Custom</text>

                    <!-- Arrow 4 -->
                    <line x1="620" y1="130" x2="660" y2="130" stroke="#00d2ff" stroke-width="2" marker-end="url(#arrowhead)"/>

                    <!-- Fine-tuning -->
                    <rect x="660" y="80" width="120" height="100" fill="rgba(0, 210, 255, 0.15)" stroke="#00d2ff" stroke-width="2" rx="8"/>
                    <text x="720" y="115" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="bold">Fine-tune</text>
                    <text x="720" y="133" text-anchor="middle" fill="#8888a0" font-size="11">Training</text>
                    <text x="720" y="153" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Validation</text>
                    <text x="720" y="168" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Checkpoints</text>

                    <!-- Arrow 5 -->
                    <line x1="780" y1="130" x2="820" y2="130" stroke="#00d2ff" stroke-width="2" marker-end="url(#arrowhead)"/>

                    <!-- Evaluation -->
                    <rect x="820" y="80" width="120" height="100" fill="rgba(108, 99, 255, 0.15)" stroke="#6c63ff" stroke-width="2" rx="8"/>
                    <text x="880" y="115" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="bold">Evaluate</text>
                    <text x="880" y="133" text-anchor="middle" fill="#8888a0" font-size="11">BLEU, chrF</text>
                    <text x="880" y="153" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Human Eval</text>
                    <text x="880" y="168" text-anchor="middle" fill="#8888a0" font-size="10" font-style="italic">Analysis</text>
                </g>

                <!-- Arrow marker definition -->
                <defs>
                    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#00d2ff"/>
                    </marker>
                </defs>
            </svg>

            <p style="text-align: center; color: var(--muted); font-size: 0.9rem;">Click each step below for detailed instructions</p>
        </section>

        <!-- Step 1: Data Collection -->
        <section class="section">
            <h2>Step 1: Data Collection for Ancient Languages</h2>
            <p>Ancient texts are sparse compared to modern data. You must be creative about sources.</p>

            <h3>Primary Data Sources</h3>
            <div class="card-grid">
                <div class="card expandable" onclick="toggleExpand(this)">
                    <h4>üìö Perseus Digital Library</h4>
                    <p>Greek & Latin texts with English translations. Over 70M words of classical texts.</p>
                    <div style="margin-top: 1rem; color: var(--accent2); font-size: 0.9rem;">
                        <p><strong>Access:</strong> www.perseus.tufts.edu</p>
                        <p><strong>Format:</strong> XML, plain text</p>
                        <p><strong>Size:</strong> ~30k classical works</p>
                        <p style="margin-top: 0.5rem; color: var(--muted); font-size: 0.85rem;">Contains texts from Homer, Plato, Ovid, Livy with parallel English translations. Perfect for supervised training.</p>
                    </div>
                </div>
                <div class="card expandable" onclick="toggleExpand(this)">
                    <h4>üî∫ CDLI - Cuneiform Database</h4>
                    <p>Sumerian & Akkadian cuneiform inscriptions with transliterations.</p>
                    <div style="margin-top: 1rem; color: var(--accent2); font-size: 0.9rem;">
                        <p><strong>Access:</strong> cdli.ucla.edu</p>
                        <p><strong>Format:</strong> JSON, TEI XML</p>
                        <p><strong>Size:</strong> ~500k artifacts</p>
                        <p style="margin-top: 0.5rem; color: var(--muted); font-size: 0.85rem;">Modern transliterations of ancient Mesopotamian texts. Challenges: sparse translations, varied conventions.</p>
                    </div>
                </div>
                <div class="card expandable" onclick="toggleExpand(this)">
                    <h4>üìú papyri.info</h4>
                    <p>Egyptian papyri with Greek and Coptic texts and translations.</p>
                    <div style="margin-top: 1rem; color: var(--accent2); font-size: 0.9rem;">
                        <p><strong>Access:</strong> papyri.info</p>
                        <p><strong>Format:</strong> XML, searchable API</p>
                        <p><strong>Size:</strong> ~100k documents</p>
                        <p style="margin-top: 0.5rem; color: var(--muted); font-size: 0.85rem;">Practical texts (letters, contracts) often with multiple language versions. Great for domain adaptation.</p>
                    </div>
                </div>
            </div>

            <h3>Parallel Corpus Format</h3>
            <p>Your training data should be aligned pairs:</p>

            <div class="data-pair">
                <div class="data-pair-lang">Latin ‚Üí English</div>
                <div class="data-pair-text"><strong>Source:</strong> "Aeneas arma virumque cano, Troiae qui primus ab oris"</div>
                <div class="data-pair-text"><strong>Target:</strong> "Arms and the man I sing, who first from Troy's far coasts"</div>
            </div>

            <div class="data-pair">
                <div class="data-pair-lang">Ancient Greek ‚Üí English</div>
                <div class="data-pair-text"><strong>Source:</strong> "œÑŒπœÇ ·ºÑœÅŒ± ·ºêœÉœÑŒπ Œ¥ŒπŒ¨œÜŒøœÅŒøœÇ œÑŒø·ø¶ ·ºÄŒΩŒ¥œÅ·Ω∏œÇ;"</div>
                <div class="data-pair-text"><strong>Target:</strong> "What then is the difference between a man?"</div>
            </div>

            <div class="data-pair">
                <div class="data-pair-lang">Sumerian ‚Üí English</div>
                <div class="data-pair-text"><strong>Source:</strong> "≈°ubba-ƒùu10 ≈°ag4-ƒùa2-ƒùu10 a-a-ƒùu10"</div>
                <div class="data-pair-text"><strong>Target:</strong> "My dream, my vision of the night, my father"</div>
            </div>

            <h3>Data Augmentation for Low-Resource Languages</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>Back-translation</h4>
                    <p>Translate English back to ancient language with intermediate model. Creates new training pairs: Eng‚ÜíOld‚ÜíEng.</p>
                </div>
                <div class="card">
                    <h4>Paraphrase</h4>
                    <p>Use GPT-4 to paraphrase English translations naturally. Keeps source parallel text identical.</p>
                </div>
                <div class="card">
                    <h4>Language Tags</h4>
                    <p>Add source/target language tags to decoder. Enables multi-script training on single model.</p>
                </div>
                <div class="card">
                    <h4>Synthetic Data</h4>
                    <p>Generate data from grammar rules & lexicons. Less natural but increases corpus size 10x.</p>
                </div>
            </div>

            <p style="margin-top: 1.5rem; color: var(--muted); font-size: 0.95rem;"><strong>Pro tip:</strong> Ancient language translation rarely has >100k sentence pairs. Consider this low-resource domain throughout.</p>
        </section>

        <!-- Step 2: Custom Tokenizer -->
        <section class="section">
            <h2>Step 2: Custom Tokenizer Training</h2>
            <p>Pre-trained tokenizers are optimized for modern English. Ancient languages need custom tokenization.</p>

            <h3>Why Custom Tokenization?</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>Script & Characters</h4>
                    <p>Ancient Greek diacritics, Old English runes, cuneiform wedges‚Äîpre-trained tokenizers don't know these.</p>
                </div>
                <div class="card">
                    <h4>Morphology</h4>
                    <p>Latin has complex inflections. Generic tokenizers split words poorly.</p>
                </div>
                <div class="card">
                    <h4>Domain Vocabulary</h4>
                    <p>Religious texts, legal documents, medical writing use different vocabularies.</p>
                </div>
                <div class="card">
                    <h4>Frequency Matters</h4>
                    <p>BPE learns merge patterns from your corpus, not general internet text.</p>
                </div>
            </div>

            <h3>Training a BPE Tokenizer</h3>
            <div class="code-block">
                <button class="code-copy" onclick="copyCode('tokenizer-code')">Copy Code</button>
                <pre id="tokenizer-code"><span class="code-line"><span class="line-number">1</span>from tokenizers import Tokenizer, normalizers, decoders
<span class="code-line"><span class="line-number">2</span>from tokenizers.models import BPE
<span class="code-line"><span class="line-number">3</span>from tokenizers.trainers import BpeTrainer
<span class="code-line"><span class="line-number">4</span>from tokenizers.pre_tokenizers import Whitespace
<span class="code-line"><span class="line-number">5</span>
<span class="code-line"><span class="line-number">6</span># Initialize tokenizer with BPE
<span class="code-line"><span class="line-number">7</span>tokenizer = Tokenizer(BPE())
<span class="code-line"><span class="line-number">8</span>tokenizer.pre_tokenizer = Whitespace()
<span class="code-line"><span class="line-number">9</span>
<span class="code-line"><span class="line-number">10</span># Trainer configuration
<span class="code-line"><span class="line-number">11</span>trainer = BpeTrainer(
<span class="code-line"><span class="line-number">12</span>    vocab_size=8000,  # Small for low-resource
<span class="code-line"><span class="line-number">13</span>    min_frequency=2,
<span class="code-line"><span class="line-number">14</span>    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[MASK]", "[PAD]"]
<span class="code-line"><span class="line-number">15</span>)
<span class="code-line"><span class="line-number">16</span>
<span class="code-line"><span class="line-number">17</span># Train on your corpus files
<span class="code-line"><span class="line-number">18</span>tokenizer.train(
<span class="code-line"><span class="line-number">19</span>    ["ancient_texts.txt", "english_translations.txt"],
<span class="code-line"><span class="line-number">20</span>    trainer=trainer
<span class="code-line"><span class="line-number">21</span>)
<span class="code-line"><span class="line-number">22</span>
<span class="code-line"><span class="line-number">23</span># Test tokenization
<span class="code-line"><span class="line-number">24</span>latin_text = "Aeneas arma virumque cano"
<span class="code-line"><span class="line-number">25</span>tokens = tokenizer.encode(latin_text)
<span class="code-line"><span class="line-number">26</span>print(f"Tokens: {tokens.tokens}")
<span class="code-line"><span class="line-number">27</span>print(f"Count: {len(tokens.tokens)}")
<span class="code-line"><span class="line-number">28</span>
<span class="code-line"><span class="line-number">29</span># Save for later use
<span class="code-line"><span class="line-number">30</span>tokenizer.save("ancient_tokenizer.json")</span></pre>
            </div>

            <h3>Tokenization Comparison</h3>
            <p>See the difference between generic and custom tokenizers:</p>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
                <div>
                    <h4 style="color: var(--accent3); margin-bottom: 1rem;">Generic BERT Tokenizer</h4>
                    <div class="code-block" style="margin: 0;">
                        <pre style="margin: 0; font-size: 0.8rem;">Text: "magistra dicit"

Tokens:
[CLS] mag ##ist ##ra dicit [SEP]

Vocab match: 40%</pre>
                    </div>
                    <p style="color: var(--muted); font-size: 0.85rem; margin-top: 1rem;">Splits morphologically. "magistra" becomes 3 tokens. Loss of word boundaries.</p>
                </div>
                <div>
                    <h4 style="color: var(--accent2); margin-bottom: 1rem;">Custom Ancient Tokenizer</h4>
                    <div class="code-block" style="margin: 0;">
                        <pre style="margin: 0; font-size: 0.8rem;">Text: "magistra dicit"

Tokens:
[CLS] magistra dicit [SEP]

Vocab match: 98%</pre>
                    </div>
                    <p style="color: var(--muted); font-size: 0.85rem; margin-top: 1rem;">Learned on your corpus. Preserves word structure. Higher OOV handling.</p>
                </div>
            </div>
        </section>

        <!-- Step 3: Model Architecture Decision -->
        <section class="section">
            <h2>Step 3: Model Architecture Selection</h2>
            <p>Choose the right architecture based on your data size:</p>

            <h3>Decision Tree</h3>
            <svg viewBox="0 0 1000 500" style="width: 100%; max-height: 500px; margin: 2rem 0;">
                <defs>
                    <style>
                        .decision-node { fill: rgba(108, 99, 255, 0.15); stroke: #6c63ff; }
                        .data-node { fill: rgba(0, 210, 255, 0.15); stroke: #00d2ff; }
                        .action-node { fill: rgba(255, 107, 107, 0.15); stroke: #ff6b6b; }
                        .text-normal { fill: #e0e0e8; font-size: 13px; text-anchor: middle; }
                        .text-small { fill: #8888a0; font-size: 11px; text-anchor: middle; }
                        .line { stroke: rgba(224, 224, 232, 0.3); stroke-width: 2; }
                    </style>
                </defs>

                <!-- Root -->
                <ellipse cx="500" cy="50" rx="80" ry="40" class="decision-node"/>
                <text x="500" y="55" class="text-normal">Data Size?</text>

                <!-- Branches -->
                <!-- Large data branch -->
                <line x1="420" y1="85" x2="250" y2="150" class="line"/>
                <rect x="150" y="150" width="200" height="80" rx="8" class="data-node"/>
                <text x="250" y="170" class="text-normal">100k+ pairs</text>
                <text x="250" y="190" class="text-small">Train from Scratch</text>
                <text x="250" y="210" class="text-small">mBART, mT5-large</text>

                <!-- Medium data branch -->
                <line x1="500" y1="90" x2="500" y2="150" class="line"/>
                <rect x="400" y="150" width="200" height="80" rx="8" class="data-node"/>
                <text x="500" y="170" class="text-normal">10k-100k pairs</text>
                <text x="500" y="190" class="text-small">Fine-tune Multilingual</text>
                <text x="500" y="210" class="text-small">mT5-base, mBART</text>

                <!-- Small data branch -->
                <line x1="580" y1="85" x2="750" y2="150" class="line"/>
                <rect x="650" y="150" width="200" height="80" rx="8" class="data-node"/>
                <text x="750" y="170" class="text-normal">< 10k pairs</text>
                <text x="750" y="190" class="text-small">Few-shot or Hybrid</text>
                <text x="750" y="210" class="text-small">GPT-4 + Fine-tune</text>

                <!-- Recommendations -->
                <!-- Large data -->
                <rect x="50" y="300" width="300" height="150" rx="8" class="action-node" stroke-width="2"/>
                <text x="200" y="330" class="text-normal" font-weight="bold">Large Data Strategy</text>
                <text x="60" y="355" class="text-small" text-anchor="start">‚Ä¢ 2-4 week training</text>
                <text x="60" y="375" class="text-small" text-anchor="start">‚Ä¢ Train encoder-decoder</text>
                <text x="60" y="395" class="text-small" text-anchor="start">‚Ä¢ Use data augmentation</text>
                <text x="60" y="415" class="text-small" text-anchor="start">‚Ä¢ Full hyperparameter tune</text>

                <!-- Medium data -->
                <rect x="350" y="300" width="300" height="150" rx="8" class="action-node" stroke-width="2"/>
                <text x="500" y="330" class="text-normal" font-weight="bold">Medium Data Strategy</text>
                <text x="360" y="355" class="text-small" text-anchor="start">‚Ä¢ 2-7 days training</text>
                <text x="360" y="375" class="text-small" text-anchor="start">‚Ä¢ Load multilingual weights</text>
                <text x="360" y="395" class="text-small" text-anchor="start">‚Ä¢ Early stopping crucial</text>
                <text x="360" y="415" class="text-small" text-anchor="start">‚Ä¢ Monitor validation closely</text>

                <!-- Small data -->
                <rect x="650" y="300" width="300" height="150" rx="8" class="action-node" stroke-width="2"/>
                <text x="800" y="330" class="text-normal" font-weight="bold">Small Data Strategy</text>
                <text x="660" y="355" class="text-small" text-anchor="start">‚Ä¢ Prompt-engineer GPT-4</text>
                <text x="660" y="375" class="text-small" text-anchor="start">‚Ä¢ Generate synthetic data</text>
                <text x="660" y="395" class="text-small" text-anchor="start">‚Ä¢ Fine-tune lightly</text>
                <text x="660" y="415" class="text-small" text-anchor="start">‚Ä¢ Few-shot in-context learning</text>

                <!-- Connections -->
                <line x1="200" y1="230" x2="200" y2="300" class="line"/>
                <line x1="500" y1="230" x2="500" y2="300" class="line"/>
                <line x1="800" y1="230" x2="800" y2="300" class="line"/>
            </svg>

            <h3>Architecture Comparison</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>mBART-50</h4>
                    <p><strong>Best for:</strong> Many language pairs (50+), medium data.</p>
                    <p style="margin-top: 0.5rem; font-size: 0.9rem; color: var(--muted);">Pre-trained on 50 language denoising. Fast to fine-tune. ~600M params.</p>
                </div>
                <div class="card">
                    <h4>mT5</h4>
                    <p><strong>Best for:</strong> Text-to-text tasks, any size data.</p>
                    <p style="margin-top: 0.5rem; font-size: 0.9rem; color: var(--muted);">Multi-lingual T5. More flexible. Comes in small/base/large sizes.</p>
                </div>
                <div class="card">
                    <h4>Custom Transformer</h4>
                    <p><strong>Best for:</strong> Large data, specific requirements.</p>
                    <p style="margin-top: 0.5rem; font-size: 0.9rem; color: var(--muted);">Full control. High compute cost. 12+ weeks training time.</p>
                </div>
                <div class="card">
                    <h4>GPT-4 Hybrid</h4>
                    <p><strong>Best for:</strong> Small data, fast iteration.</p>
                    <p style="margin-top: 0.5rem; font-size: 0.9rem; color: var(--muted);">Prompt-based few-shot. Cost per query. No fine-tuning needed.</p>
                </div>
            </div>

            <p style="margin-top: 1.5rem; color: var(--muted);"><strong>Recommendation for ancient languages:</strong> Start with mT5-base if you have 10k+ pairs, or GPT-4 if you have <5k. Both have been battle-tested on low-resource tasks.</p>
        </section>

        <!-- Step 4: Training Script -->
        <section class="section">
            <h2>Step 4: Complete Training Script</h2>
            <p>Production-ready code for fine-tuning mT5 on ancient language translation:</p>

            <div class="code-block">
                <button class="code-copy" onclick="copyCode('training-code')">Copy Code</button>
                <pre id="training-code"><span class="code-line"><span class="line-number">1</span>#!/usr/bin/env python3
<span class="code-line"><span class="line-number">2</span>"""Fine-tune mT5 for ancient language translation"""
<span class="code-line"><span class="line-number">3</span>import torch
<span class="code-line"><span class="line-number">4</span>from datasets import load_dataset, Dataset
<span class="code-line"><span class="line-number">5</span>from transformers import (
<span class="code-line"><span class="line-number">6</span>    AutoTokenizer, AutoModelForSeq2SeqLM,
<span class="code-line"><span class="line-number">7</span>    Seq2SeqTrainingArguments, Seq2SeqTrainer,
<span class="code-line"><span class="line-number">8</span>    DataCollatorForSeq2Seq
<span class="code-line"><span class="line-number">9</span>)
<span class="code-line"><span class="line-number">10</span>import numpy as np
<span class="code-line"><span class="line-number">11</span>
<span class="code-line"><span class="line-number">12</span># Configuration
<span class="code-line"><span class="line-number">13</span>MODEL_NAME = "google/mt5-base"
<span class="code-line"><span class="line-number">14</span>SOURCE_LANG = "lat"  # Latin
<span class="code-line"><span class="line-number">15</span>TARGET_LANG = "en"
<span class="code-line"><span class="line-number">16</span>BATCH_SIZE = 16
<span class="code-line"><span class="line-number">17</span>EPOCHS = 3
<span class="code-line"><span class="line-number">18</span>
<span class="code-line"><span class="line-number">19</span># Load tokenizer & model
<span class="code-line"><span class="line-number">20</span>tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
<span class="code-line"><span class="line-number">21</span>model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
<span class="code-line"><span class="line-number">22</span>
<span class="code-line"><span class="line-number">23</span># Load your parallel corpus
<span class="code-line"><span class="line-number">24</span>def load_data(source_file, target_file):
<span class="code-line"><span class="line-number">25</span>    with open(source_file) as f:
<span class="code-line"><span class="line-number">26</span>        sources = [l.strip() for l in f]
<span class="code-line"><span class="line-number">27</span>    with open(target_file) as f:
<span class="code-line"><span class="line-number">28</span>        targets = [l.strip() for l in f]
<span class="code-line"><span class="line-number">29</span>    return Dataset.from_dict({
<span class="code-line"><span class="line-number">30</span>        "translation": [{"lat": s, "en": t} for s, t in zip(sources, targets)]
<span class="code-line"><span class="line-number">31</span>    })
<span class="code-line"><span class="line-number">32</span>
<span class="code-line"><span class="line-number">33</span>dataset = load_data("latin_texts.txt", "english_texts.txt")
<span class="code-line"><span class="line-number">34</span>train_test = dataset.train_test_split(test_size=0.1)
<span class="code-line"><span class="line-number">35</span>
<span class="code-line"><span class="line-number">36</span># Preprocess
<span class="code-line"><span class="line-number">37</span>def preprocess(batch):
<span class="code-line"><span class="line-number">38</span>    model_inputs = tokenizer(
<span class="code-line"><span class="line-number">39</span>        batch["translation"][0][SOURCE_LANG],
<span class="code-line"><span class="line-number">40</span>        max_length=128, truncation=True
<span class="code-line"><span class="line-number">41</span>    )
<span class="code-line"><span class="line-number">42</span>    labels = tokenizer(
<span class="code-line"><span class="line-number">43</span>        batch["translation"][0][TARGET_LANG],
<span class="code-line"><span class="line-number">44</span>        max_length=128, truncation=True
<span class="code-line"><span class="line-number">45</span>    )
<span class="code-line"><span class="line-number">46</span>    model_inputs["labels"] = labels["input_ids"]
<span class="code-line"><span class="line-number">47</span>    return model_inputs
<span class="code-line"><span class="line-number">48</span>
<span class="code-line"><span class="line-number">49</span>train_data = train_test["train"].map(preprocess, batched=True)
<span class="code-line"><span class="line-number">50</span>eval_data = train_test["test"].map(preprocess, batched=True)
<span class="code-line"><span class="line-number">51</span>
<span class="code-line"><span class="line-number">52</span># Training args
<span class="code-line"><span class="line-number">53</span>training_args = Seq2SeqTrainingArguments(
<span class="code-line"><span class="line-number">54</span>    output_dir="./ancient_translation_model",
<span class="code-line"><span class="line-number">54</span>    num_train_epochs=EPOCHS,
<span class="code-line"><span class="line-number">55</span>    per_device_train_batch_size=BATCH_SIZE,
<span class="code-line"><span class="line-number">56</span>    per_device_eval_batch_size=32,
<span class="code-line"><span class="line-number">57</span>    warmup_steps=500,
<span class="code-line"><span class="line-number">58</span>    weight_decay=0.01,
<span class="code-line"><span class="line-number">59</span>    save_strategy="epoch",
<span class="code-line"><span class="line-number">60</span>    eval_strategy="epoch",
<span class="code-line"><span class="line-number">61</span>    load_best_model_at_end=True,
<span class="code-line"><span class="line-number">62</span>    predict_with_generate=True,
<span class="code-line"><span class="line-number">63</span>)
<span class="code-line"><span class="line-number">64</span>
<span class="code-line"><span class="line-number">65</span># Trainer
<span class="code-line"><span class="line-number">66</span>trainer = Seq2SeqTrainer(
<span class="code-line"><span class="line-number">67</span>    model=model,
<span class="code-line"><span class="line-number">68</span>    args=training_args,
<span class="code-line"><span class="line-number">69</span>    train_dataset=train_data,
<span class="code-line"><span class="line-number">70</span>    eval_dataset=eval_data,
<span class="code-line"><span class="line-number">71</span>    tokenizer=tokenizer,
<span class="code-line"><span class="line-number">72</span>    data_collator=DataCollatorForSeq2Seq(tokenizer, model),
<span class="code-line"><span class="line-number">73</span>)
<span class="code-line"><span class="line-number">73</span>
<span class="code-line"><span class="line-number">74</span># Train
<span class="code-line"><span class="line-number">75</span>trainer.train()
<span class="code-line"><span class="line-number">76</span>model.save_pretrained("./ancient_model_final")
<span class="code-line"><span class="line-number">77</span>print("‚úì Training complete!")</span></pre>
            </div>

            <h3>Training Loss Curve (Typical)</h3>
            <div class="loss-curve-container">
                <svg viewBox="0 0 800 300" style="width: 100%;">
                    <!-- Grid -->
                    <defs>
                        <pattern id="grid" width="40" height="30" patternUnits="userSpaceOnUse">
                            <path d="M 40 0 L 0 0 0 30" fill="none" stroke="rgba(108, 99, 255, 0.1)" stroke-width="0.5"/>
                        </pattern>
                    </defs>
                    <rect width="800" height="300" fill="url(#grid)"/>

                    <!-- Axes -->
                    <line x1="60" y1="250" x2="760" y2="250" stroke="#6c63ff" stroke-width="2"/>
                    <line x1="60" y1="30" x2="60" y2="250" stroke="#6c63ff" stroke-width="2"/>

                    <!-- Labels -->
                    <text x="400" y="290" text-anchor="middle" fill="#8888a0" font-size="12">Epoch</text>
                    <text x="20" y="150" text-anchor="middle" fill="#8888a0" font-size="12" transform="rotate(-90 20 150)">Loss</text>

                    <!-- Loss curve (smooth exponential decay) -->
                    <path d="M 60 70 Q 200 130, 280 160 Q 360 185, 440 205 Q 520 215, 600 225 Q 680 230, 760 235"
                          fill="none" stroke="#00d2ff" stroke-width="3" stroke-linecap="round"/>

                    <!-- Data points -->
                    <circle cx="60" cy="70" r="4" fill="#ff6b6b"/>
                    <circle cx="280" cy="160" r="4" fill="#ff6b6b"/>
                    <circle cx="440" cy="205" r="4" fill="#ff6b6b"/>
                    <circle cx="600" cy="225" r="4" fill="#ff6b6b"/>
                    <circle cx="760" cy="235" r="4" fill="#ff6b6b"/>

                    <!-- Annotations -->
                    <text x="40" y="50" fill="#e0e0e8" font-size="11" font-weight="bold">5.2</text>
                    <text x="40" y="170" fill="#e0e0e8" font-size="11" font-weight="bold">2.1</text>
                    <text x="40" y="250" fill="#e0e0e8" font-size="11" font-weight="bold">0</text>
                </svg>
                <p style="text-align: center; color: var(--muted); font-size: 0.9rem; margin-top: 1rem;">Typical 3-epoch training for 10k sentence pairs. Initial steep drop, then gradual convergence.</p>
            </div>

            <h3>Monitoring During Training</h3>
            <p>Watch these metrics:</p>
            <div class="card-grid">
                <div class="card">
                    <h4>Perplexity</h4>
                    <p>Exponential of average loss. Lower is better. Target: <5 for ancient text.</p>
                </div>
                <div class="card">
                    <h4>BLEU Score</h4>
                    <p>Translation quality metric (0-100). Expect 15-25 for low-resource pairs.</p>
                </div>
                <div class="card">
                    <h4>Validation Loss</h4>
                    <p>Should decrease with training. Increasing = overfitting. Use early stopping at +5% increase.</p>
                </div>
                <div class="card">
                    <h4>Gradient Norm</h4>
                    <p>Watch for NaN or explosion. Target: 0.5-2.0. Adjust learning rate if unstable.</p>
                </div>
            </div>
        </section>

        <!-- Step 5: Evaluation -->
        <section class="section">
            <h2>Step 5: Evaluation Metrics</h2>
            <p>How do you know if your ancient language translator is good?</p>

            <h3>BLEU Score Explained</h3>
            <p>BLEU (Bilingual Evaluation Understudy) measures n-gram overlap between translation and reference:</p>

            <div class="bleu-calculator">
                <h4 style="margin-bottom: 1rem;">Interactive BLEU Calculator</h4>

                <div>
                    <label style="display: block; margin-bottom: 0.5rem; color: var(--accent2);">Reference Translation:</label>
                    <textarea id="bleu-reference" placeholder="the quick brown fox jumps" style="width: 100%; height: 60px;"></textarea>
                </div>

                <div style="margin-top: 1rem;">
                    <label style="display: block; margin-bottom: 0.5rem; color: var(--accent2);">Candidate Translation:</label>
                    <textarea id="bleu-candidate" placeholder="the quick brown fox" style="width: 100%; height: 60px;"></textarea>
                </div>

                <button onclick="calculateBLEU()" style="margin-top: 1rem; width: 100%;">Calculate BLEU</button>

                <div id="bleu-result-panel" style="display: none; margin-top: 1.5rem;">
                    <div class="bleu-result">
                        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 1rem;">
                            <div>
                                <div class="bleu-info">BLEU Score (1-4gram)</div>
                                <div class="bleu-score" id="bleu-score-value">0</div>
                            </div>
                            <div>
                                <div class="bleu-info">Interpretation</div>
                                <div id="bleu-interpretation" style="font-size: 1.1rem; color: var(--accent2); margin-top: 0.5rem;">‚Äî</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <h3>BLEU Benchmarks for Translation</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>0-10: Unintelligible</h4>
                    <p>Almost no meaningful overlap. Model still learning or severely undertrained.</p>
                </div>
                <div class="card">
                    <h4>10-25: Poor</h4>
                    <p>Basic competence. Low-resource languages typically land here. Useful for drafts only.</p>
                </div>
                <div class="card">
                    <h4>25-50: Good</h4>
                    <p>Readable translations. May need post-editing. Acceptable for many applications.</p>
                </div>
                <div class="card">
                    <h4>50+: Excellent</h4>
                    <p>Professional quality. Rare for low-resource language pairs. Requires massive data or strong pre-training.</p>
                </div>
            </div>

            <h3>Other Evaluation Metrics</h3>
            <div class="card-grid">
                <div class="card expandable" onclick="toggleExpand(this)">
                    <h4>chrF (Character F-score)</h4>
                    <p>Compares character n-grams instead of word n-grams. Better for morphologically rich languages like Latin.</p>
                    <div style="margin-top: 1rem; color: var(--accent2); font-size: 0.9rem;">
                        <p><strong>Range:</strong> 0-100 (like BLEU)</p>
                        <p><strong>Advantage:</strong> Handles spelling variations & inflections</p>
                        <p style="margin-top: 0.5rem; color: var(--muted); font-size: 0.85rem;">Great for ancient languages where morphology matters more than exact word boundaries.</p>
                    </div>
                </div>
                <div class="card expandable" onclick="toggleExpand(this)">
                    <h4>METEOR</h4>
                    <p>Considers synonyms and stemming. More lenient than BLEU.</p>
                    <div style="margin-top: 1rem; color: var(--accent2); font-size: 0.9rem;">
                        <p><strong>Range:</strong> 0-1 (0-100%)</p>
                        <p><strong>Advantage:</strong> Semantic awareness</p>
                        <p style="margin-top: 0.5rem; color: var(--muted); font-size: 0.85rem;">Slower to compute but better at capturing human judgment on ancient text translation.</p>
                    </div>
                </div>
                <div class="card expandable" onclick="toggleExpand(this)">
                    <h4>TER (Translation Error Rate)</h4>
                    <p>Minimum edit distance (Levenshtein). Lower is better.</p>
                    <div style="margin-top: 1rem; color: var(--accent2); font-size: 0.9rem;">
                        <p><strong>Range:</strong> 0-100% (% of edits needed)</p>
                        <p><strong>Advantage:</strong> Clear, intuitive meaning</p>
                        <p style="margin-top: 0.5rem; color: var(--muted); font-size: 0.85rem;">Easy to explain to non-ML stakeholders: "30% error rate means 30% word edits needed to perfect".</p>
                    </div>
                </div>
                <div class="card expandable" onclick="toggleExpand(this)">
                    <h4>Human Evaluation</h4>
                    <p>Linguists score translations on fluency, adequacy, and cultural accuracy.</p>
                    <div style="margin-top: 1rem; color: var(--accent2); font-size: 0.9rem;">
                        <p><strong>Scale:</strong> 1-5 (poor to excellent)</p>
                        <p><strong>Advantage:</strong> Gold standard</p>
                        <p style="margin-top: 0.5rem; color: var(--muted); font-size: 0.85rem;">Expensive but essential for ancient languages where meaning preservation is critical. Use for 100-200 samples.</p>
                    </div>
                </div>
            </div>

            <h3>Complete Evaluation Script</h3>
            <div class="code-block">
                <button class="code-copy" onclick="copyCode('eval-code')">Copy Code</button>
                <pre id="eval-code"><span class="code-line"><span class="line-number">1</span>from datasets import load_metric
<span class="code-line"><span class="line-number">2</span>import numpy as np
<span class="code-line"><span class="line-number">3</span>
<span class="code-line"><span class="line-number">4</span># Load metrics
<span class="code-line"><span class="line-number">5</span>bleu = load_metric("bleu")
<span class="code-line"><span class="line-number">6</span>chrf = load_metric("chrf")
<span class="code-line"><span class="line-number">7</span>
<span class="code-line"><span class="line-number">8</span># Generate predictions
<span class="code-line"><span class="line-number">9</span>predictions = model.generate(
<span class="code-line"><span class="line-number">10</span>    input_ids=inputs["input_ids"],
<span class="code-line"><span class="line-number">11</span>    max_length=128, num_beams=5
<span class="code-line"><span class="line-number">12</span>)
<span class="code-line"><span class="line-number">13</span>
<span class="code-line"><span class="line-number">14</span># Decode
<span class="code-line"><span class="line-number">15</span>decoded_preds = tokenizer.batch_decode(
<span class="code-line"><span class="line-number">16</span>    predictions, skip_special_tokens=True
<span class="code-line"><span class="line-number">17</span>)
<span class="code-line"><span class="line-number">18</span>decoded_labels = tokenizer.batch_decode(
<span class="code-line"><span class="line-number">19</span>    labels, skip_special_tokens=True
<span class="code-line"><span class="line-number">20</span>)
<span class="code-line"><span class="line-number">21</span>
<span class="code-line"><span class="line-number">22</span># Compute metrics
<span class="code-line"><span class="line-number">23</span>bleu_result = bleu.compute(
<span class="code-line"><span class="line-number">24</span>    predictions=decoded_preds,
<span class="code-line"><span class="line-number">25</span>    references=[[ref] for ref in decoded_labels]
<span class="code-line"><span class="line-number">26</span>)
<span class="code-line"><span class="line-number">27</span>chrf_result = chrf.compute(
<span class="code-line"><span class="line-number">28</span>    predictions=decoded_preds,
<span class="code-line"><span class="line-number">29</span>    references=decoded_labels
<span class="code-line"><span class="line-number">30</span>)
<span class="code-line"><span class="line-number">31</span>
<span class="code-line"><span class="line-number">32</span># Print results
<span class="code-line"><span class="line-number">33</span>print(f"BLEU:  {bleu_result['bleu']:.2f}")
<span class="code-line"><span class="line-number">34</span>print(f"chrF:  {chrf_result['score']:.2f}")
<span class="code-line"><span class="line-number">35</span>print("\nExample predictions:")
<span class="code-line"><span class="line-number">36</span>for i in range(5):
<span class="code-line"><span class="line-number">37</span>    print(f"  Ref: {decoded_labels[i]}")
<span class="code-line"><span class="line-number">38</span>    print(f"  Pred: {decoded_preds[i]}\n")</span></pre>
            </div>
        </section>

        <!-- Course Completion -->
        <section class="section">
            <h2>üéì You Did It!</h2>

            <div class="certificate">
                <div class="certificate-title">Certificate of Completion</div>
                <div class="certificate-subtitle">NLP Fundamentals to Advanced Applications</div>
                <p style="color: var(--text); font-size: 1.1rem; margin-bottom: 2rem;">You have successfully completed all 10 modules and hands-on projects</p>
                <div class="badge-row">
                    <div class="badge">
                        <div class="badge-icon">üèÜ</div>
                        <div class="badge-text">Foundations</div>
                    </div>
                    <div class="badge">
                        <div class="badge-icon">‚öôÔ∏è</div>
                        <div class="badge-text">Architecture</div>
                    </div>
                    <div class="badge">
                        <div class="badge-icon">üöÄ</div>
                        <div class="badge-text">Implementation</div>
                    </div>
                    <div class="badge">
                        <div class="badge-icon">üéØ</div>
                        <div class="badge-text">Applications</div>
                    </div>
                </div>
            </div>

            <h3>Your Journey: All 10 Modules</h3>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">1</div>
                        <h4>Language Fundamentals</h4>
                        <p>Tokenization, embeddings, and word vectors. The building blocks of everything that follows.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">2</div>
                        <h4>Attention & Transformers</h4>
                        <p>Self-attention mechanism and the Transformer architecture. The foundation of modern NLP.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">3</div>
                        <h4>Transfer Learning</h4>
                        <p>Pre-training and fine-tuning. Why we don't train from scratch anymore.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">4</div>
                        <h4>Language Models</h4>
                        <p>From BERT to GPT. Autoregressive vs masked modeling. In-context learning.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">5</div>
                        <h4>Sequence-to-Sequence Models</h4>
                        <p>Encoder-decoder architectures. Translation, summarization, and more.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">6</div>
                        <h4>Retrieval & Knowledge</h4>
                        <p>RAG systems, semantic search, and grounding language models with facts.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">7</div>
                        <h4>Evaluation & Metrics</h4>
                        <p>How to measure quality beyond BLEU. Human evaluation and edge cases.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">8</div>
                        <h4>Fine-tuning & Optimization</h4>
                        <p>Parameter-efficient training. LoRA, adapters, and quantization.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">9</div>
                        <h4>Deployment & Production</h4>
                        <p>Serving models at scale. Latency, throughput, and reliability.</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-number">10</div>
                        <h4>Hands-On Project</h4>
                        <p>Text classification and ancient language translation. Real code, real challenges.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Where to Go Next -->
        <section class="section">
            <h2>Where to Go Next</h2>
            <p>You've finished the course, but your NLP journey is just beginning. Here are the best next steps:</p>

            <h3>Research Papers to Read</h3>
            <div class="resources-grid">
                <div class="resource-card">
                    <div class="resource-icon">üìÑ</div>
                    <h4>Attention Is All You Need</h4>
                    <p>Vaswani et al. 2017. The original Transformer paper. Changed NLP forever.</p>
                    <a href="#">Read on arXiv ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üìÑ</div>
                    <h4>BERT: Pre-training of Deep Bidirectional Transformers</h4>
                    <p>Devlin et al. 2018. Masked language modeling and why pre-training works.</p>
                    <a href="#">Read on arXiv ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üìÑ</div>
                    <h4>Language Models are Few-Shot Learners</h4>
                    <p>Brown et al. 2020. GPT-3. In-context learning without fine-tuning.</p>
                    <a href="#">Read on arXiv ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üìÑ</div>
                    <h4>LoRA: Low-Rank Adaptation</h4>
                    <p>Hu et al. 2021. Fine-tune billion-parameter models efficiently.</p>
                    <a href="#">Read on arXiv ‚Üí</a>
                </div>
            </div>

            <h3>Datasets to Explore</h3>
            <div class="resources-grid">
                <div class="resource-card">
                    <div class="resource-icon">üìä</div>
                    <h4>HuggingFace Datasets</h4>
                    <p>40k+ datasets for every NLP task. Classification, QA, translation, more.</p>
                    <a href="#">Browse ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üìö</div>
                    <h4>WNUT Datasets</h4>
                    <p>Workshop on Noisy User-Generated Text. Real-world, messy data.</p>
                    <a href="#">Explore ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üåç</div>
                    <h4>WMT Translation Shared Task</h4>
                    <p>Machine translation benchmarks across 100+ language pairs.</p>
                    <a href="#">View ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üìñ</div>
                    <h4>SQuAD</h4>
                    <p>Stanford Question Answering Dataset. 100k+ QA pairs on Wikipedia.</p>
                    <a href="#">Download ‚Üí</a>
                </div>
            </div>

            <h3>Communities & Conferences</h3>
            <div class="resources-grid">
                <div class="resource-card">
                    <div class="resource-icon">üë•</div>
                    <h4>HuggingFace Community</h4>
                    <p>Active Discord, forums, and model hub. Share your work. Get feedback.</p>
                    <a href="#">Join ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üé§</div>
                    <h4>ACL (Annual Conference)</h4>
                    <p>Association for Computational Linguistics. Premier NLP research venue.</p>
                    <a href="#">Website ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">ü§ñ</div>
                    <h4>NeurIPS & ICML</h4>
                    <p>General ML conferences with strong NLP tracks. Broader audience.</p>
                    <a href="#">Explore ‚Üí</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">üìù</div>
                    <h4>arXiv NLP Feed</h4>
                    <p>1000+ papers posted per month. Follow arxiv.org/list/cs.CL</p>
                    <a href="#">Subscribe ‚Üí</a>
                </div>
            </div>

            <h3>Next Steps for Your Ancient Language Project</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>1. Collect & Clean Data</h4>
                    <p>Gather 5k-10k parallel sentence pairs from Perseus, CDLI, or papyri.info. Normalize scripts and formats.</p>
                </div>
                <div class="card">
                    <h4>2. Train Custom Tokenizer</h4>
                    <p>Use SentencePiece or HuggingFace Tokenizers. Vocab size 4k-8k works well for ancient languages.</p>
                </div>
                <div class="card">
                    <h4>3. Select & Fine-tune Model</h4>
                    <p>Start with mT5-base. Train for 3-5 epochs on a single GPU. Monitor BLEU and validation loss.</p>
                </div>
                <div class="card">
                    <h4>4. Evaluate & Iterate</h4>
                    <p>Compute BLEU, chrF, and get human feedback on 100-200 samples. Data augmentation if BLEU < 20.</p>
                </div>
                <div class="card">
                    <h4>5. Deploy & Share</h4>
                    <p>Push to HuggingFace Hub. Write a model card. Share in ancient language communities on Reddit, Mastodon.</p>
                </div>
                <div class="card">
                    <h4>6. Improve Continuously</h4>
                    <p>Collect user feedback. Curate high-quality corrections. Fine-tune incrementally. Never stop learning.</p>
                </div>
            </div>
        </section>

        <!-- Final Thoughts -->
        <section class="section" style="background: linear-gradient(135deg, rgba(108, 99, 255, 0.2) 0%, rgba(255, 107, 107, 0.1) 100%); border-left: 4px solid var(--accent2);">
            <h2>Final Thoughts</h2>
            <p style="font-size: 1.1rem; line-height: 1.8; margin-bottom: 1.5rem;">
                NLP has transformed from a niche research field to the core of AI. The techniques you've learned in this course power:
            </p>
            <ul style="color: var(--muted); font-size: 1rem; line-height: 1.8; margin-left: 1.5rem; margin-bottom: 1.5rem;">
                <li>‚úì ChatGPT, Claude, Llama‚Äîconversational AI</li>
                <li>‚úì Google Translate, DeepL‚Äîmachine translation</li>
                <li>‚úì Alexa, Siri, Google Assistant‚Äîvoice understanding</li>
                <li>‚úì Content moderation, spam detection‚Äîsafety</li>
                <li>‚úì Medical record analysis, legal document review‚Äîenterprise</li>
                <li>‚úì Preserving ancient texts, translating endangered languages‚Äîcultural heritage</li>
            </ul>
            <p style="font-size: 1.1rem; line-height: 1.8;">
                Your ancient language translation project is more than a coding exercise. You're building tools to preserve humanity's intellectual heritage. Languages that have been silent for millennia could speak again. That's the power of NLP.
            </p>
            <p style="margin-top: 1.5rem; color: var(--accent2); font-size: 1rem; font-weight: 600;">
                Now go build something amazing. üöÄ
            </p>
        </section>

        <!-- Navigation Footer -->
        <div class="nav-footer">
            <a href="09-finetuning.html">‚Üê Fine-Tuning & Optimization</a>
            <a href="index.html">Back to Course Home</a>
        </div>
    </div>

    <script>
        // Code explanations
        const explanations = {
            imports: {
                title: "Imports & Setup",
                content: "We load HuggingFace Transformers, Datasets, and PyTorch. These are the industry-standard libraries for NLP. No need to implement anything from scratch."
            },
            model: {
                title: "Loading Pre-trained Model",
                content: "AutoModelForSequenceClassification loads DistilBERT with sentiment classification head. The model has already learned English from 16GB of text. We're just adapting it for our specific task."
            },
            data: {
                title: "Dataset Loading",
                content: "IMDB is a classic sentiment dataset: 50k movie reviews labeled positive (1) or negative (0). We use 1k training samples for speed. Production uses all 50k."
            },
            tokenize: {
                title: "Tokenization",
                content: "The tokenizer breaks text into subword tokens that the model understands. DistilBERT's tokenizer was trained on English Wikipedia. Every token maps to an ID in the vocabulary."
            },
            training: {
                title: "Training Setup",
                content: "TrainingArguments configure learning rate, batch size, number of epochs, and evaluation strategy. Trainer handles all the complexity: backward pass, optimization, checkpointing, mixed precision."
            },
            eval: {
                title: "Prediction & Evaluation",
                content: "After training, we evaluate on the test set and make predictions. For 'This movie is absolutely fantastic!' the model outputs logits [negative_score, positive_score]. Argmax picks the class."
            }
        };

        function showExplanation(element, key) {
            // Remove active class from all sections
            document.querySelectorAll('.code-section').forEach(el => el.classList.remove('active'));
            element.classList.add('active');

            // Update explanation panel
            const exp = explanations[key];
            const panel = document.getElementById('explanation-panel');
            panel.innerHTML = `
                <h4>${exp.title}</h4>
                <p>${exp.content}</p>
            `;
        }

        function copyCode(elementId) {
            const code = document.getElementById(elementId);
            const text = code.textContent.replace(/^\d+\s*/gm, '');
            navigator.clipboard.writeText(text);
            alert('Code copied to clipboard!');
        }

        // Sentiment classification demo
        const sentimentExamples = [
            { text: "This movie was absolutely terrible. Complete waste of time.", sentiment: "Negative", confidence: 0.94 },
            { text: "Amazing! One of the best films I've ever seen.", sentiment: "Positive", confidence: 0.97 },
            { text: "It was okay. Some parts were good, others not so much.", sentiment: "Positive", confidence: 0.58 },
            { text: "Brilliant cinematography and outstanding performances throughout.", sentiment: "Positive", confidence: 0.96 },
            { text: "Boring and predictable. Couldn't even finish it.", sentiment: "Negative", confidence: 0.91 },
            { text: "A masterpiece. Truly unforgettable experience.", sentiment: "Positive", confidence: 0.98 },
            { text: "Not worth your time or money. Disappointing from start to finish.", sentiment: "Negative", confidence: 0.93 },
            { text: "Enjoyed it quite a bit. Would recommend to friends.", sentiment: "Positive", confidence: 0.87 },
            { text: "Mediocre at best. Could have been much better.", sentiment: "Negative", confidence: 0.71 },
            { text: "Absolutely loved every second. Planning to watch again!", sentiment: "Positive", confidence: 0.99 }
        ];

        function initializeSentimentExamples() {
            const container = document.getElementById('sentiment-examples');
            sentimentExamples.forEach((example, index) => {
                const badgeClass = example.sentiment === 'Positive' ? 'sentiment-positive' : 'sentiment-negative';
                const item = document.createElement('div');
                item.className = 'sentiment-item';
                item.onclick = () => {
                    document.getElementById('review-input').value = example.text;
                };
                item.innerHTML = `
                    <div class="sentiment-item-text">"${example.text}"</div>
                    <div class="sentiment-item-result">
                        <span class="sentiment-badge ${badgeClass}">${example.sentiment}</span>
                        <span class="sentiment-confidence">${(example.confidence * 100).toFixed(0)}%</span>
                    </div>
                `;
                container.appendChild(item);
            });
        }

        function classifyReview() {
            const text = document.getElementById('review-input').value;
            if (!text.trim()) {
                alert('Please enter a review');
                return;
            }

            // Simple heuristic classification (in reality, this would be the model)
            const positiveWords = ['amazing', 'excellent', 'fantastic', 'brilliant', 'love', 'perfect', 'great', 'best', 'wonderful'];
            const negativeWords = ['terrible', 'awful', 'bad', 'waste', 'boring', 'disappointing', 'horrible', 'worst', 'hate'];

            const lowerText = text.toLowerCase();
            let posScore = 0, negScore = 0;

            positiveWords.forEach(word => {
                posScore += (lowerText.match(new RegExp(word, 'g')) || []).length;
            });
            negativeWords.forEach(word => {
                negScore += (lowerText.match(new RegExp(word, 'g')) || []).length;
            });

            const total = posScore + negScore || 1;
            const confidence = Math.max(posScore, negScore) / total;
            const sentiment = posScore > negScore ? 'Positive' : 'Negative';

            document.getElementById('sentiment-label').textContent = sentiment;
            document.getElementById('sentiment-label').className = 'result-main result-' + sentiment.toLowerCase();
            const fill = document.getElementById('confidence-fill');
            fill.style.width = (confidence * 100) + '%';
            fill.textContent = (confidence * 100).toFixed(0) + '%';
            document.getElementById('demo-result').style.display = 'block';
        }

        // BLEU Calculator
        function calculateBLEU() {
            const reference = document.getElementById('bleu-reference').value.toLowerCase().split(/\s+/);
            const candidate = document.getElementById('bleu-candidate').value.toLowerCase().split(/\s+/);

            if (reference.length === 0 || candidate.length === 0) {
                alert('Please enter both reference and candidate translations');
                return;
            }

            // Simple BLEU calculation (1-gram precision)
            const matches = candidate.filter(word => reference.includes(word)).length;
            const precision = matches / candidate.length;
            const bleuScore = (precision * 100).toFixed(1);

            let interpretation = 'Poor';
            if (bleuScore > 50) interpretation = 'Excellent';
            else if (bleuScore > 25) interpretation = 'Good';
            else if (bleuScore > 10) interpretation = 'Fair';

            document.getElementById('bleu-score-value').textContent = bleuScore;
            document.getElementById('bleu-interpretation').textContent = interpretation;
            document.getElementById('bleu-result-panel').style.display = 'block';
        }

        // Toggle expand for cards
        function toggleExpand(element) {
            element.classList.toggle('expanded');
        }

        // Scroll-triggered animations
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        });

        document.querySelectorAll('.section').forEach(el => {
            observer.observe(el);
        });

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            initializeSentimentExamples();
        });
    </script>
</body>
</html>
