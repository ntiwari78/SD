<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 07: BERT - NLP Course</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    :root {
      --bg: #0a0a0f;
      --surface: #12121a;
      --card: #1a1a2e;
      --border: #2a2a3e;
      --accent: #6c63ff;
      --accent2: #00d2ff;
      --accent3: #ff6b6b;
      --text: #e0e0e8;
      --muted: #8888a0;
    }

    body {
      background-color: var(--bg);
      color: var(--text);
      font-family: 'Inter', system-ui, sans-serif;
      line-height: 1.6;
      overflow-x: hidden;
    }

    /* Sticky Navigation */
    nav {
      position: sticky;
      top: 0;
      z-index: 1000;
      background: linear-gradient(180deg, var(--surface) 0%, rgba(18, 18, 26, 0.8) 100%);
      backdrop-filter: blur(10px);
      border-bottom: 1px solid var(--border);
      padding: 1.5rem 2rem;
    }

    nav a {
      color: var(--accent);
      text-decoration: none;
      font-weight: 500;
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      transition: color 0.3s ease;
    }

    nav a:hover {
      color: var(--accent2);
    }

    /* Fade-in Animation */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .fade-in {
      animation: fadeInUp 0.8s ease-out;
    }

    /* Hero Section */
    .hero {
      padding: 6rem 2rem;
      text-align: center;
      background: linear-gradient(135deg, rgba(108, 99, 255, 0.1) 0%, rgba(0, 210, 255, 0.05) 100%);
      border-bottom: 1px solid var(--border);
    }

    .hero h1 {
      font-size: 3.5rem;
      margin-bottom: 0.5rem;
      background: linear-gradient(135deg, var(--accent2), var(--accent));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .hero .subtitle {
      font-size: 1.5rem;
      color: var(--accent2);
      margin-bottom: 1rem;
    }

    .hero .meta {
      display: flex;
      justify-content: center;
      gap: 2rem;
      margin-top: 2rem;
      color: var(--muted);
      font-size: 0.95rem;
    }

    .meta-item {
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    /* Main Container */
    .container {
      max-width: 1000px;
      margin: 0 auto;
      padding: 3rem 2rem;
    }

    section {
      margin-bottom: 5rem;
    }

    h2 {
      font-size: 2.2rem;
      margin-bottom: 1.5rem;
      color: var(--accent);
      border-left: 4px solid var(--accent2);
      padding-left: 1rem;
    }

    h3 {
      font-size: 1.3rem;
      margin-top: 1.5rem;
      margin-bottom: 0.8rem;
      color: var(--accent2);
    }

    p {
      margin-bottom: 1rem;
      color: var(--text);
      line-height: 1.8;
    }

    /* Card Styles */
    .card {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 2rem;
      margin-bottom: 1.5rem;
      transition: all 0.3s ease;
    }

    .card:hover {
      border-color: var(--accent);
      box-shadow: 0 0 20px rgba(108, 99, 255, 0.2);
    }

    /* Timeline */
    .timeline {
      display: flex;
      gap: 1rem;
      overflow-x: auto;
      padding: 1.5rem 0;
      margin: 2rem 0;
    }

    .timeline-item {
      flex: 0 0 200px;
      background: var(--card);
      border: 2px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      cursor: pointer;
      transition: all 0.3s ease;
      text-align: center;
    }

    .timeline-item:hover {
      border-color: var(--accent);
      transform: translateY(-5px);
      box-shadow: 0 10px 30px rgba(108, 99, 255, 0.2);
    }

    .timeline-item.active {
      background: linear-gradient(135deg, var(--accent), var(--accent2));
      border-color: var(--accent2);
      color: var(--bg);
    }

    .timeline-item h4 {
      font-size: 1rem;
      margin-bottom: 0.5rem;
    }

    .timeline-item p {
      font-size: 0.85rem;
      margin: 0;
    }

    /* Interactive Sections */
    .interactive {
      background: var(--surface);
      border: 2px solid var(--border);
      border-radius: 8px;
      padding: 2rem;
      margin: 2rem 0;
    }

    .button {
      background: linear-gradient(135deg, var(--accent), var(--accent2));
      color: var(--bg);
      border: none;
      padding: 0.8rem 1.5rem;
      border-radius: 6px;
      font-size: 0.95rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      margin-right: 1rem;
      margin-bottom: 1rem;
    }

    .button:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 25px rgba(108, 99, 255, 0.3);
    }

    .button:active {
      transform: translateY(0);
    }

    /* Context Flow Visualization */
    .context-demo {
      margin: 2rem 0;
      padding: 1.5rem;
      background: var(--surface);
      border-radius: 8px;
      border: 1px solid var(--border);
    }

    .sentence-tokens {
      display: flex;
      flex-wrap: wrap;
      gap: 0.8rem;
      margin: 1.5rem 0;
      justify-content: center;
    }

    .token {
      background: var(--card);
      border: 2px solid var(--border);
      padding: 0.8rem 1.2rem;
      border-radius: 6px;
      font-weight: 500;
      cursor: pointer;
      transition: all 0.3s ease;
      position: relative;
    }

    .token:hover {
      border-color: var(--accent);
      transform: scale(1.05);
    }

    .token.masked {
      background: linear-gradient(135deg, var(--accent3), rgba(255, 107, 107, 0.3));
      border-color: var(--accent3);
    }

    .token.left-context {
      border-color: #00ff00;
      background: rgba(0, 255, 0, 0.1);
    }

    .token.right-context {
      border-color: #ffaa00;
      background: rgba(255, 170, 0, 0.1);
    }

    .token.both-context {
      border-color: var(--accent2);
      background: rgba(0, 210, 255, 0.15);
    }

    .prediction {
      background: var(--card);
      padding: 1rem;
      border-radius: 6px;
      margin: 0.8rem 0;
      border-left: 4px solid var(--accent2);
    }

    .prediction-word {
      font-weight: 600;
      color: var(--accent2);
    }

    .prediction-bar {
      background: var(--border);
      height: 20px;
      border-radius: 4px;
      margin-top: 0.5rem;
      overflow: hidden;
    }

    .prediction-fill {
      background: linear-gradient(90deg, var(--accent), var(--accent2));
      height: 100%;
      display: flex;
      align-items: center;
      justify-content: flex-end;
      padding-right: 0.5rem;
      font-size: 0.8rem;
      font-weight: 600;
    }

    /* Task Cards */
    .task-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 1.5rem;
      margin: 2rem 0;
    }

    .task-card {
      background: var(--card);
      border: 2px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      cursor: pointer;
      transition: all 0.3s ease;
      text-align: center;
    }

    .task-card:hover {
      border-color: var(--accent);
      transform: translateY(-5px);
    }

    .task-card.active {
      background: linear-gradient(135deg, rgba(108, 99, 255, 0.2), rgba(0, 210, 255, 0.1));
      border-color: var(--accent2);
    }

    .task-card h4 {
      margin-bottom: 0.8rem;
      color: var(--accent2);
    }

    .task-card p {
      font-size: 0.85rem;
      margin: 0;
    }

    /* NSP Demo */
    .nsp-pair {
      background: var(--surface);
      border: 1px solid var(--border);
      padding: 1.5rem;
      border-radius: 8px;
      margin: 1.5rem 0;
    }

    .sentence {
      background: var(--card);
      padding: 1rem;
      margin: 0.8rem 0;
      border-radius: 6px;
      border-left: 4px solid var(--accent);
    }

    .nsp-result {
      margin-top: 1.5rem;
      padding: 1rem;
      background: var(--card);
      border-radius: 6px;
      font-weight: 600;
    }

    .nsp-result.next {
      border-left: 4px solid #00ff00;
      color: #00ff00;
    }

    .nsp-result.not-next {
      border-left: 4px solid var(--accent3);
      color: var(--accent3);
    }

    /* Architecture SVG */
    .architecture {
      display: flex;
      justify-content: center;
      margin: 2rem 0;
    }

    svg {
      max-width: 100%;
      height: auto;
    }

    /* Code Block */
    .code-block {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
      overflow-x: auto;
      font-family: 'Courier New', monospace;
      font-size: 0.9rem;
      line-height: 1.5;
    }

    .code-block code {
      color: var(--accent2);
    }

    .code-keyword {
      color: var(--accent);
    }

    .code-string {
      color: #00ff00;
    }

    .code-comment {
      color: var(--muted);
    }

    /* Footer Navigation */
    .nav-footer {
      display: flex;
      justify-content: space-between;
      padding: 3rem 2rem;
      border-top: 1px solid var(--border);
      margin-top: 5rem;
    }

    .nav-footer a {
      color: var(--accent);
      text-decoration: none;
      font-weight: 500;
      transition: color 0.3s ease;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .nav-footer a:hover {
      color: var(--accent2);
    }

    /* Responsive */
    @media (max-width: 768px) {
      .hero h1 {
        font-size: 2.2rem;
      }

      .hero .meta {
        flex-direction: column;
        gap: 1rem;
      }

      .container {
        padding: 2rem 1rem;
      }

      h2 {
        font-size: 1.8rem;
      }

      .timeline-item {
        flex: 0 0 150px;
      }

      .task-grid {
        grid-template-columns: 1fr;
      }

      nav {
        padding: 1rem 1.5rem;
      }
    }

    /* Scrolling reveal animation */
    .reveal {
      opacity: 0;
      transform: translateY(30px);
      transition: all 0.8s ease;
    }

    .reveal.active {
      opacity: 1;
      transform: translateY(0);
    }
  </style>
</head>
<body>
  <nav>
    <a href="index.html">‚Üê Back to Course</a>
  </nav>

  <!-- Hero Section -->
  <section class="hero fade-in">
    <h1>Module 07: BERT</h1>
    <p class="subtitle">Reading in Both Directions</p>
    <div class="meta">
      <div class="meta-item">‚è±Ô∏è ~25 minutes</div>
      <div class="meta-item">üè¢ Google Research, 2018</div>
      <div class="meta-item">üìä Advanced Transformer</div>
    </div>
  </section>

  <div class="container">
    <!-- The Pre-training Revolution -->
    <section class="reveal">
      <h2>The Pre-training Revolution</h2>
      <p>BERT fundamentally changed how we approach NLP. Before BERT, you'd need to train a model from scratch for each new task. After BERT, you train once and fine-tune everywhere.</p>

      <div class="timeline">
        <div class="timeline-item" onclick="highlightTimeline(0)">
          <h4>Pre-BERT</h4>
          <p>Train from scratch for each task</p>
        </div>
        <div class="timeline-item" onclick="highlightTimeline(1)">
          <h4>BERT</h4>
          <p>Pre-train once, fine-tune everywhere</p>
        </div>
        <div class="timeline-item" onclick="highlightTimeline(2)">
          <h4>Impact</h4>
          <p>Massive improvement in efficiency and performance</p>
        </div>
      </div>

      <svg class="architecture" viewBox="0 0 600 250" style="max-width: 100%; height: auto;">
        <!-- Pre-BERT -->
        <g>
          <rect x="20" y="20" width="120" height="80" fill="none" stroke="#8888a0" stroke-width="2" rx="6"/>
          <text x="80" y="70" text-anchor="middle" fill="#e0e0e8" font-size="14" font-weight="600">Task Data</text>

          <line x1="80" y1="100" x2="80" y2="130" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>

          <rect x="30" y="130" width="100" height="80" fill="none" stroke="#ff6b6b" stroke-width="2" rx="6"/>
          <text x="80" y="180" text-anchor="middle" fill="#ff6b6b" font-size="14" font-weight="600">Train from scratch</text>
          <text x="80" y="198" text-anchor="middle" fill="#ff6b6b" font-size="12">(Slow & Limited)</text>
        </g>

        <!-- Post-BERT -->
        <g>
          <rect x="240" y="20" width="120" height="80" fill="none" stroke="#00d2ff" stroke-width="2" rx="6"/>
          <text x="300" y="55" text-anchor="middle" fill="#e0e0e8" font-size="14" font-weight="600">Huge</text>
          <text x="300" y="75" text-anchor="middle" fill="#e0e0e8" font-size="14" font-weight="600">Corpus</text>

          <line x1="300" y1="100" x2="300" y2="130" stroke="#00d2ff" stroke-width="2" marker-end="url(#arrowhead)"/>

          <rect x="250" y="130" width="100" height="80" fill="none" stroke="#00d2ff" stroke-width="2" rx="6"/>
          <text x="300" y="165" text-anchor="middle" fill="#00d2ff" font-size="14" font-weight="600">Pre-train</text>
          <text x="300" y="183" text-anchor="middle" fill="#00d2ff" font-size="14" font-weight="600">BERT</text>
        </g>

        <!-- Fine-tune -->
        <g>
          <rect x="460" y="20" width="120" height="80" fill="none" stroke="#6c63ff" stroke-width="2" rx="6"/>
          <text x="520" y="55" text-anchor="middle" fill="#e0e0e8" font-size="14" font-weight="600">Task-</text>
          <text x="520" y="75" text-anchor="middle" fill="#e0e0e8" font-size="14" font-weight="600">Specific Data</text>

          <line x1="520" y1="100" x2="520" y2="130" stroke="#6c63ff" stroke-width="2" marker-end="url(#arrowhead)"/>

          <rect x="470" y="130" width="100" height="80" fill="none" stroke="#6c63ff" stroke-width="2" rx="6"/>
          <text x="520" y="165" text-anchor="middle" fill="#6c63ff" font-size="14" font-weight="600">Fine-tune</text>
          <text x="520" y="183" text-anchor="middle" fill="#6c63ff" font-size="14" font-weight="600">(Fast & Effective)</text>
        </g>

        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
            <polygon points="0 0, 10 5, 0 10" fill="#8888a0"/>
          </marker>
        </defs>
      </svg>
    </section>

    <!-- Why Bidirectional? -->
    <section class="reveal">
      <h2>Why Bidirectional?</h2>
      <p>This is BERT's key insight. Most language models read text left-to-right (like GPT). But human understanding works differently‚Äîwe understand words using context from <em>both directions</em>.</p>

      <div class="context-demo">
        <p style="text-align: center; margin-bottom: 1.5rem; color: var(--accent2);">
          "I went to the bank to deposit my ___"
        </p>

        <h4 style="color: var(--text); margin-bottom: 1rem;">Left-to-Right Model (GPT-style)</h4>
        <p style="color: var(--muted); margin-bottom: 0.8rem;">Can only see words that came before:</p>
        <div class="sentence-tokens">
          <div class="token">I</div>
          <div class="token">went</div>
          <div class="token">to</div>
          <div class="token">the</div>
          <div class="token">bank</div>
          <div class="token">to</div>
          <div class="token">deposit</div>
          <div class="token">my</div>
          <div class="token" style="background: rgba(255, 107, 107, 0.3); border-color: #ff6b6b;">?</div>
        </div>
        <p style="color: var(--muted); font-size: 0.9rem;">‚ùå Ambiguous: "bank" could mean financial institution OR river bank</p>

        <h4 style="color: var(--text); margin: 1.5rem 0 1rem;">Bidirectional Model (BERT)</h4>
        <p style="color: var(--muted); margin-bottom: 0.8rem;">Can see words from BOTH directions:</p>
        <div class="sentence-tokens">
          <div class="token both-context">I</div>
          <div class="token both-context">went</div>
          <div class="token both-context">to</div>
          <div class="token both-context">the</div>
          <div class="token both-context">bank</div>
          <div class="token both-context">to</div>
          <div class="token both-context">deposit</div>
          <div class="token both-context">my</div>
          <div class="token both-context">???</div>
        </div>
        <p style="color: var(--muted); font-size: 0.9rem;">‚úÖ Clear: "deposit" signals financial institution ‚Üí "money" is the best prediction</p>
      </div>

      <div class="card">
        <h3>Key Insight</h3>
        <p>BERT reads the entire sentence at once, so each word can see left context AND right context. This bidirectional understanding is why BERT is so much better at capturing meaning.</p>
      </div>
    </section>

    <!-- Masked Language Modeling -->
    <section class="reveal">
      <h2>Masked Language Modeling (MLM)</h2>
      <p>BERT's main pre-training objective. It randomly masks 15% of tokens and learns to predict them‚Äîforcing the model to understand bidirectional context.</p>

      <div class="interactive">
        <p style="margin-bottom: 1.5rem;">Click a word to see predictions (or click "Mask Random Words" to try another sentence):</p>

        <div id="mlmSentenceContainer" style="margin-bottom: 1.5rem;">
          <!-- Will be populated by JavaScript -->
        </div>

        <button class="button" onclick="maskRandomWords()">Mask Random Words</button>
        <button class="button" onclick="resetMLM()" style="background: var(--border); color: var(--text);">Reset</button>

        <div id="mlmPredictions" style="margin-top: 1.5rem;">
          <!-- Predictions will appear here -->
        </div>
      </div>

      <div class="card">
        <h3>MLM Training Strategy</h3>
        <p>For each masked token:</p>
        <ul style="margin-left: 1.5rem; margin-top: 0.8rem;">
          <li><strong>80% of the time:</strong> Replace with [MASK] token</li>
          <li><strong>10% of the time:</strong> Replace with random word</li>
          <li><strong>10% of the time:</strong> Keep original (unchanged)</li>
        </ul>
        <p style="margin-top: 1rem; color: var(--muted);">Why? This prevents BERT from "cheating" by just learning to ignore [MASK] tokens. It must truly understand context.</p>
      </div>
    </section>

    <!-- Next Sentence Prediction -->
    <section class="reveal">
      <h2>Next Sentence Prediction (NSP)</h2>
      <p>BERT's second pre-training objective. Given two sentences, predict if they're actually consecutive in the corpus or just random pairs.</p>

      <div class="interactive">
        <p style="margin-bottom: 1.5rem;">Click "Check Prediction" to see if BERT thinks these sentences are consecutive:</p>

        <div id="nspContainer">
          <!-- Will be populated by JavaScript -->
        </div>

        <button class="button" onclick="nextNSPExample()">Next Example</button>
        <button class="button" onclick="resetNSP()" style="background: var(--border); color: var(--text);">Reset</button>
      </div>

      <div class="card">
        <h3>How NSP Works</h3>
        <p>Two sentences are fed with a [SEP] token between them. BERT learns to classify with [CLS] token:</p>
        <ul style="margin-left: 1.5rem; margin-top: 0.8rem;">
          <li><strong>IsNext:</strong> Sentences are consecutive in corpus</li>
          <li><strong>NotNext:</strong> Sentences are random pairs</li>
        </ul>
        <p style="margin-top: 1rem; color: var(--muted);">Note: Later models found NSP less important than MLM, but it helped BERT understand discourse relationships.</p>
      </div>
    </section>

    <!-- BERT Architecture -->
    <section class="reveal">
      <h2>BERT Architecture Breakdown</h2>
      <p>BERT stacks embeddings and transformer encoders. Let's break down each component.</p>

      <svg class="architecture" viewBox="0 0 800 400" style="max-width: 100%;">
        <!-- Input Layer -->
        <g>
          <text x="100" y="25" text-anchor="middle" fill="#00d2ff" font-size="14" font-weight="600">Input Tokens</text>
          <rect x="30" y="35" width="140" height="60" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
          <text x="100" y="70" text-anchor="middle" fill="#e0e0e8" font-size="12">[CLS] The cat sat</text>
        </g>

        <!-- Token Embeddings -->
        <g>
          <text x="280" y="25" text-anchor="middle" fill="#6c63ff" font-size="14" font-weight="600">Token Embeddings</text>
          <rect x="220" y="35" width="120" height="60" fill="none" stroke="#6c63ff" stroke-width="2" rx="4"/>
          <text x="280" y="70" text-anchor="middle" fill="#e0e0e8" font-size="11">768-dim vectors</text>
        </g>

        <!-- Segment Embeddings -->
        <g>
          <text x="460" y="25" text-anchor="middle" fill="#6c63ff" font-size="14" font-weight="600">Segment Embeddings</text>
          <rect x="390" y="35" width="140" height="60" fill="none" stroke="#6c63ff" stroke-width="2" rx="4"/>
          <text x="460" y="70" text-anchor="middle" fill="#e0e0e8" font-size="11">Sentence A or B</text>
        </g>

        <!-- Position Embeddings -->
        <g>
          <text x="650" y="25" text-anchor="middle" fill="#6c63ff" font-size="14" font-weight="600">Position Embeddings</text>
          <rect x="570" y="35" width="160" height="60" fill="none" stroke="#6c63ff" stroke-width="2" rx="4"/>
          <text x="650" y="70" text-anchor="middle" fill="#e0e0e8" font-size="11">Token position (pos 0-511)</text>
        </g>

        <!-- Arrows to combination -->
        <line x1="100" y1="95" x2="400" y2="130" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
        <line x1="280" y1="95" x2="400" y2="130" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
        <line x1="460" y1="95" x2="400" y2="130" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
        <line x1="650" y1="95" x2="400" y2="130" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>

        <!-- Combined Input -->
        <g>
          <rect x="300" y="130" width="200" height="60" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
          <text x="400" y="160" text-anchor="middle" fill="#00d2ff" font-size="13" font-weight="600">Combined Input (768-dim)</text>
        </g>

        <!-- Encoder Stack -->
        <g>
          <text x="400" y="235" text-anchor="middle" fill="#ffaa00" font-size="14" font-weight="600">Transformer Encoder Stack</text>

          <rect x="280" y="250" width="240" height="100" fill="none" stroke="#ffaa00" stroke-width="2" rx="4"/>
          <text x="400" y="275" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="600">12 Layers (Base) or 24 Layers (Large)</text>
          <text x="400" y="295" text-anchor="middle" fill="#e0e0e8" font-size="12">Each layer: Multi-head Self-Attention</text>
          <text x="400" y="315" text-anchor="middle" fill="#e0e0e8" font-size="12">+ Feed-Forward Networks</text>
          <text x="400" y="335" text-anchor="middle" fill="#e0e0e8" font-size="12">110M params (Base)</text>
        </g>

        <!-- Output -->
        <g>
          <line x1="400" y1="350" x2="400" y2="370" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <text x="400" y="395" text-anchor="middle" fill="#00d2ff" font-size="13" font-weight="600">Output: [CLS] representation</text>
        </g>

        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
            <polygon points="0 0, 10 5, 0 10" fill="#8888a0"/>
          </marker>
        </defs>
      </svg>

      <div class="card">
        <h3>Special Tokens</h3>
        <ul style="margin-left: 1.5rem; margin-top: 0.8rem;">
          <li><strong>[CLS]:</strong> Classification token at the beginning. Used for sequence-level tasks (sent to classification head)</li>
          <li><strong>[SEP]:</strong> Separator token between sentences A and B</li>
          <li><strong>[MASK]:</strong> Replaces tokens during pre-training for MLM objective</li>
          <li><strong>[UNK]:</strong> Unknown token for out-of-vocabulary words</li>
        </ul>
      </div>

      <h3>BERT-Base vs BERT-Large</h3>
      <div class="task-grid">
        <div class="card">
          <h4>BERT-Base</h4>
          <p>12 layers, 768 hidden size, 12 attention heads</p>
          <p style="color: var(--accent2); margin-top: 0.8rem; font-weight: 600;">110M parameters</p>
        </div>
        <div class="card">
          <h4>BERT-Large</h4>
          <p>24 layers, 1024 hidden size, 16 attention heads</p>
          <p style="color: var(--accent2); margin-top: 0.8rem; font-weight: 600;">340M parameters</p>
        </div>
      </div>
    </section>

    <!-- Fine-tuning BERT -->
    <section class="reveal">
      <h2>Fine-tuning BERT</h2>
      <p>The beauty of BERT: the same pre-trained model adapts to any downstream task by adding a small task-specific head.</p>

      <div class="task-grid">
        <div class="task-card" onclick="showTaskArchitecture('classification')">
          <h4>Classification</h4>
          <p>Sentiment, intent, entailment, etc.</p>
          <p style="color: var(--muted); margin-top: 0.8rem; font-size: 0.8rem;">Click to see architecture</p>
        </div>
        <div class="task-card" onclick="showTaskArchitecture('ner')">
          <h4>NER</h4>
          <p>Named Entity Recognition</p>
          <p style="color: var(--muted); margin-top: 0.8rem; font-size: 0.8rem;">Click to see architecture</p>
        </div>
        <div class="task-card" onclick="showTaskArchitecture('qa')">
          <h4>Question Answering</h4>
          <p>SQuAD-style span prediction</p>
          <p style="color: var(--muted); margin-top: 0.8rem; font-size: 0.8rem;">Click to see architecture</p>
        </div>
        <div class="task-card" onclick="showTaskArchitecture('similarity')">
          <h4>Sentence Similarity</h4>
          <p>Semantic similarity/paraphrase</p>
          <p style="color: var(--muted); margin-top: 0.8rem; font-size: 0.8rem;">Click to see architecture</p>
        </div>
      </div>

      <div id="taskArchitecture" style="margin-top: 2rem;">
        <!-- Architecture diagrams will be inserted here -->
      </div>
    </section>

    <!-- BERT Family Tree -->
    <section class="reveal">
      <h2>BERT Family Tree</h2>
      <p>BERT sparked a revolution. Many variants improved on it:</p>

      <div class="timeline" style="margin-bottom: 2rem;">
        <div class="timeline-item" onclick="selectBertVariant('bert')">
          <h4>BERT</h4>
          <p>2018</p>
          <p style="font-size: 0.75rem; margin-top: 0.5rem;">Original bidirectional</p>
        </div>
        <div class="timeline-item" onclick="selectBertVariant('roberta')">
          <h4>RoBERTa</h4>
          <p>2019</p>
          <p style="font-size: 0.75rem; margin-top: 0.5rem;">Better pre-training</p>
        </div>
        <div class="timeline-item" onclick="selectBertVariant('albert')">
          <h4>ALBERT</h4>
          <p>2019</p>
          <p style="font-size: 0.75rem; margin-top: 0.5rem;">Parameter reduction</p>
        </div>
        <div class="timeline-item" onclick="selectBertVariant('distilbert')">
          <h4>DistilBERT</h4>
          <p>2019</p>
          <p style="font-size: 0.75rem; margin-top: 0.5rem;">Smaller & faster</p>
        </div>
        <div class="timeline-item" onclick="selectBertVariant('debert')">
          <h4>DeBERTa</h4>
          <p>2020</p>
          <p style="font-size: 0.75rem; margin-top: 0.5rem;">Disentangled attention</p>
        </div>
      </div>

      <div id="bertVariantInfo" class="card">
        <!-- Variant info will be inserted here -->
      </div>
    </section>

    <!-- Python Code Examples -->
    <section class="reveal">
      <h2>Python Implementation</h2>
      <p>Using HuggingFace Transformers library (the standard way to work with BERT):</p>

      <h3>1. Loading BERT Tokenizer & Model</h3>
      <div class="code-block">
<span class="code-keyword">from</span> transformers <span class="code-keyword">import</span> AutoTokenizer, AutoModel

<span class="code-comment"># Load pre-trained BERT</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="code-string">"bert-base-uncased"</span>)
model = AutoModel.from_pretrained(<span class="code-string">"bert-base-uncased"</span>)

<span class="code-comment"># Tokenize input</span>
text = <span class="code-string">"I went to the bank to deposit my money"</span>
inputs = tokenizer(text, return_tensors=<span class="code-string">"pt"</span>)
outputs = model(**inputs)

<span class="code-comment"># outputs.last_hidden_state has shape [batch, seq_len, 768]</span>
      </div>

      <h3>2. Masked Language Model Predictions</h3>
      <div class="code-block">
<span class="code-keyword">from</span> transformers <span class="code-keyword">import</span> pipeline

<span class="code-comment"># Use MLM pipeline for predictions</span>
unmasker = pipeline(<span class="code-string">'fill-mask'</span>, model=<span class="code-string">'bert-base-uncased'</span>)

result = unmasker(<span class="code-string">"I went to the bank to deposit my [MASK]"</span>)

<span class="code-comment"># Returns top 5 predictions with scores</span>
<span class="code-keyword">for</span> pred <span class="code-keyword">in</span> result:
    <span class="code-keyword">print</span>(pred[<span class="code-string">'token_str'</span>], pred[<span class="code-string">'score'</span>])
      </div>

      <h3>3. Text Classification (Fine-tuned)</h3>
      <div class="code-block">
<span class="code-keyword">from</span> transformers <span class="code-keyword">import</span> pipeline

<span class="code-comment"># Sentiment classification</span>
classifier = pipeline(<span class="code-string">'sentiment-analysis'</span>,
                      model=<span class="code-string">'distilbert-base-uncased-finetuned-sst-2-english'</span>)

result = classifier(<span class="code-string">"This movie was absolutely fantastic!"</span>)
<span class="code-comment"># Returns: [{'label': 'POSITIVE', 'score': 0.9987}]</span>
      </div>

      <h3>4. Feature Extraction</h3>
      <div class="code-block">
<span class="code-keyword">import</span> torch

<span class="code-comment"># Extract [CLS] token representation (sentence embedding)</span>
inputs = tokenizer(<span class="code-string">"BERT is great"</span>, return_tensors=<span class="code-string">"pt"</span>)
outputs = model(**inputs)

<span class="code-comment"># CLS token is at position 0</span>
cls_embedding = outputs.last_hidden_state[0, 0, :]  <span class="code-comment"># Shape: [768]</span>

<span class="code-comment"># Use as sentence embedding for similarity tasks</span>
      </div>
    </section>

    <!-- Footer Navigation -->
    <div class="nav-footer">
      <a href="06-transformer.html">‚Üê Transformers</a>
      <a href="08-gpt.html">GPT ‚Üí</a>
    </div>
  </div>

  <script>
    // MLM Data
    const mlmSentences = [
      {
        tokens: ['[CLS]', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '[SEP]'],
        predictions: {
          2: [{ word: 'quick', conf: 0.92 }, { word: 'big', conf: 0.05 }, { word: 'small', conf: 0.03 }],
          3: [{ word: 'brown', conf: 0.88 }, { word: 'red', conf: 0.08 }, { word: 'dark', conf: 0.04 }],
          4: [{ word: 'fox', conf: 0.95 }, { word: 'dog', conf: 0.03 }, { word: 'cat', conf: 0.02 }],
          8: [{ word: 'lazy', conf: 0.91 }, { word: 'sleepy', conf: 0.06 }, { word: 'tired', conf: 0.03 }]
        }
      },
      {
        tokens: ['[CLS]', 'I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'my', 'money', '[SEP]'],
        predictions: {
          5: [{ word: 'bank', conf: 0.94 }, { word: 'river', conf: 0.04 }, { word: 'store', conf: 0.02 }],
          9: [{ word: 'money', conf: 0.93 }, { word: 'cash', conf: 0.05 }, { word: 'check', conf: 0.02 }]
        }
      },
      {
        tokens: ['[CLS]', 'BERT', 'is', 'a', 'revolutionary', 'transformer', 'model', '[SEP]'],
        predictions: {
          1: [{ word: 'BERT', conf: 0.96 }, { word: 'GPT', conf: 0.02 }, { word: 'T5', conf: 0.02 }],
          3: [{ word: 'a', conf: 0.89 }, { word: 'the', conf: 0.08 }, { word: 'an', conf: 0.03 }],
          5: [{ word: 'transformer', conf: 0.92 }, { word: 'neural', conf: 0.05 }, { word: 'encoder', conf: 0.03 }]
        }
      }
    ];

    let currentMLMSentence = 0;
    let maskedTokens = [];

    function initMLM() {
      maskRandomWords();
    }

    function maskRandomWords() {
      const sentence = mlmSentences[currentMLMSentence];
      maskedTokens = [];

      // Randomly mask 15% of tokens (excluding special tokens)
      for (let i = 1; i < sentence.tokens.length - 1; i++) {
        if (Math.random() < 0.15 && sentence.predictions[i]) {
          maskedTokens.push(i);
        }
      }

      if (maskedTokens.length === 0) {
        maskedTokens = [Object.keys(sentence.predictions)[0]];
      }

      renderMLMSentence();
      document.getElementById('mlmPredictions').innerHTML = '';
    }

    function renderMLMSentence() {
      const sentence = mlmSentences[currentMLMSentence];
      let html = '<div class="sentence-tokens">';

      sentence.tokens.forEach((token, idx) => {
        let className = 'token';
        if (maskedTokens.includes(idx)) {
          className += ' masked';
        }

        const displayText = maskedTokens.includes(idx) ? '[MASK]' : token;
        html += `<div class="${className}" onclick="showPrediction(${idx})">${displayText}</div>`;
      });

      html += '</div>';
      document.getElementById('mlmSentenceContainer').innerHTML = html;
    }

    function showPrediction(idx) {
      const sentence = mlmSentences[currentMLMSentence];
      if (!sentence.predictions[idx]) return;

      let html = '<h4 style="color: var(--accent2); margin-bottom: 1rem;">Top Predictions for position ' + idx + ' "' + sentence.tokens[idx] + '"</h4>';

      sentence.predictions[idx].forEach(pred => {
        const percentage = (pred.conf * 100).toFixed(1);
        html += `
          <div class="prediction">
            <div class="prediction-word">${pred.word}</div>
            <div class="prediction-bar">
              <div class="prediction-fill" style="width: ${percentage}%">${percentage}%</div>
            </div>
          </div>
        `;
      });

      document.getElementById('mlmPredictions').innerHTML = html;
    }

    function resetMLM() {
      currentMLMSentence = (currentMLMSentence + 1) % mlmSentences.length;
      maskRandomWords();
    }

    // NSP Data
    const nspExamples = [
      {
        sentenceA: "The foundation of large language models is the transformer architecture.",
        sentenceB: "Transformers use self-attention mechanisms to process text.",
        isNext: true
      },
      {
        sentenceA: "Machine learning has revolutionized many industries.",
        sentenceB: "Pizza is a delicious Italian dish made with dough and toppings.",
        isNext: false
      },
      {
        sentenceA: "BERT was released by Google in 2018 as a breakthrough in NLP.",
        sentenceB: "It introduced bidirectional pre-training using masked language modeling.",
        isNext: true
      }
    ];

    let currentNSPExample = 0;
    let nspAnswerShown = false;

    function renderNSP() {
      const example = nspExamples[currentNSPExample];
      let html = `
        <div class="nsp-pair">
          <div class="sentence">${example.sentenceA}</div>
          <div style="text-align: center; color: var(--accent); margin: 1rem 0; font-weight: 600;">[SEP]</div>
          <div class="sentence">${example.sentenceB}</div>
          <button class="button" style="margin-top: 1rem;" onclick="checkNSP()">Check Prediction</button>
          <div id="nspResult"></div>
        </div>
      `;
      document.getElementById('nspContainer').innerHTML = html;
      nspAnswerShown = false;
    }

    function checkNSP() {
      const example = nspExamples[currentNSPExample];
      const resultDiv = document.getElementById('nspResult');

      if (!nspAnswerShown) {
        const className = example.isNext ? 'next' : 'not-next';
        const label = example.isNext ? '‚úì IsNext' : '‚úó NotNext';
        const explanation = example.isNext
          ? 'These sentences are consecutive from the same document.'
          : 'These sentences are from different documents (random pairing).';

        resultDiv.innerHTML = `
          <div class="nsp-result ${className}">
            <div style="margin-bottom: 0.5rem;">${label}</div>
            <div style="font-size: 0.9rem; font-weight: 400;">${explanation}</div>
          </div>
        `;
        nspAnswerShown = true;
      }
    }

    function nextNSPExample() {
      currentNSPExample = (currentNSPExample + 1) % nspExamples.length;
      renderNSP();
    }

    function resetNSP() {
      currentNSPExample = 0;
      renderNSP();
    }

    // Task Architecture
    function showTaskArchitecture(task) {
      const taskCards = document.querySelectorAll('.task-card');
      taskCards.forEach(card => card.classList.remove('active'));
      event.target.closest('.task-card').classList.add('active');

      let html = '<svg class="architecture" viewBox="0 0 600 350" style="max-width: 100%;">';

      if (task === 'classification') {
        html += `
          <g>
            <rect x="200" y="20" width="200" height="60" fill="none" stroke="#6c63ff" stroke-width="2" rx="4"/>
            <text x="300" y="55" text-anchor="middle" fill="#e0e0e8" font-size="14" font-weight="600">Input: Full Text</text>
          </g>
          <line x1="300" y1="80" x2="300" y2="110" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="110" width="200" height="60" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="140" text-anchor="middle" fill="#e0e0e8" font-size="12">Pre-trained BERT</text>
            <text x="300" y="160" text-anchor="middle" fill="#e0e0e8" font-size="12">(frozen or lightly tuned)</text>
          </g>
          <line x1="300" y1="170" x2="300" y2="200" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="200" width="200" height="60" fill="none" stroke="#ff6b6b" stroke-width="2" rx="4"/>
            <text x="300" y="225" text-anchor="middle" fill="#e0e0e8" font-size="12">[CLS] Token ‚Üí Dense Layer</text>
            <text x="300" y="245" text-anchor="middle" fill="#e0e0e8" font-size="12">Output: Class Label</text>
          </g>
          <line x1="300" y1="260" x2="300" y2="290" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="220" y="290" width="160" height="40" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="317" text-anchor="middle" fill="#00d2ff" font-size="13" font-weight="600">Sentiment: POSITIVE</text>
          </g>
        `;
      } else if (task === 'ner') {
        html += `
          <g>
            <rect x="200" y="20" width="200" height="60" fill="none" stroke="#6c63ff" stroke-width="2" rx="4"/>
            <text x="300" y="55" text-anchor="middle" fill="#e0e0e8" font-size="14" font-weight="600">Input: "John lives in NYC"</text>
          </g>
          <line x1="300" y1="80" x2="300" y2="110" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="110" width="200" height="60" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="140" text-anchor="middle" fill="#e0e0e8" font-size="12">Pre-trained BERT</text>
            <text x="300" y="160" text-anchor="middle" fill="#e0e0e8" font-size="12">(output for each token)</text>
          </g>
          <line x1="300" y1="170" x2="300" y2="200" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="200" width="200" height="60" fill="none" stroke="#ff6b6b" stroke-width="2" rx="4"/>
            <text x="300" y="225" text-anchor="middle" fill="#e0e0e8" font-size="12">Token Classifier Head</text>
            <text x="300" y="245" text-anchor="middle" fill="#e0e0e8" font-size="11">(per-token labels)</text>
          </g>
          <line x1="300" y1="260" x2="300" y2="290" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="180" y="290" width="240" height="40" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="317" text-anchor="middle" fill="#00d2ff" font-size="12" font-weight="600">John(PERSON) lives in NYC(LOCATION)</text>
          </g>
        `;
      } else if (task === 'qa') {
        html += `
          <g>
            <rect x="150" y="20" width="300" height="60" fill="none" stroke="#6c63ff" stroke-width="2" rx="4"/>
            <text x="300" y="50" text-anchor="middle" fill="#e0e0e8" font-size="13" font-weight="600">Q: Where was BERT created? | Context: BERT was created at Google</text>
          </g>
          <line x1="300" y1="80" x2="300" y2="110" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="110" width="200" height="60" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="140" text-anchor="middle" fill="#e0e0e8" font-size="12">Pre-trained BERT</text>
            <text x="300" y="160" text-anchor="middle" fill="#e0e0e8" font-size="12">(per-token outputs)</text>
          </g>
          <line x1="300" y1="170" x2="300" y2="200" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="200" width="200" height="60" fill="none" stroke="#ff6b6b" stroke-width="2" rx="4"/>
            <text x="300" y="230" text-anchor="middle" fill="#e0e0e8" font-size="12">Predict span: start_pos & end_pos</text>
          </g>
          <line x1="300" y1="260" x2="300" y2="290" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="220" y="290" width="160" height="40" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="317" text-anchor="middle" fill="#00d2ff" font-size="12" font-weight="600">Answer: "Google"</text>
          </g>
        `;
      } else if (task === 'similarity') {
        html += `
          <g>
            <rect x="150" y="20" width="300" height="60" fill="none" stroke="#6c63ff" stroke-width="2" rx="4"/>
            <text x="300" y="50" text-anchor="middle" fill="#e0e0e8" font-size="12" font-weight="600">Sent 1: "Cats are animals" | Sent 2: "Felines are creatures"</text>
          </g>
          <line x1="300" y1="80" x2="300" y2="110" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="110" width="200" height="60" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="140" text-anchor="middle" fill="#e0e0e8" font-size="12">Pre-trained BERT</text>
            <text x="300" y="160" text-anchor="middle" fill="#e0e0e8" font-size="12">Extract [CLS] for both</text>
          </g>
          <line x1="300" y1="170" x2="300" y2="200" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="200" y="200" width="200" height="60" fill="none" stroke="#ff6b6b" stroke-width="2" rx="4"/>
            <text x="300" y="230" text-anchor="middle" fill="#e0e0e8" font-size="12">Compute cosine similarity</text>
          </g>
          <line x1="300" y1="260" x2="300" y2="290" stroke="#8888a0" stroke-width="2" marker-end="url(#arrowhead)"/>
          <g>
            <rect x="220" y="290" width="160" height="40" fill="none" stroke="#00d2ff" stroke-width="2" rx="4"/>
            <text x="300" y="317" text-anchor="middle" fill="#00d2ff" font-size="12" font-weight="600">Similarity Score: 0.85</text>
          </g>
        `;
      }

      html += `
        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
            <polygon points="0 0, 10 5, 0 10" fill="#8888a0"/>
          </marker>
        </defs>
      </svg>`;

      document.getElementById('taskArchitecture').innerHTML = html;
    }

    // BERT Variants
    const bertVariants = {
      bert: {
        name: 'BERT (2018)',
        description: 'Bidirectional Encoder Representations from Transformers by Google Research. The original that started it all.',
        keyFeatures: [
          'Bidirectional pre-training (left + right context)',
          'MLM (Masked Language Modeling) objective',
          'NSP (Next Sentence Prediction) objective',
          '110M parameters (Base), 340M (Large)',
          'Trained on Wikipedia + BookCorpus'
        ]
      },
      roberta: {
        name: 'RoBERTa (2019)',
        description: 'Robustly Optimized BERT Pretraining Approach. Improved BERT with better pre-training.',
        keyFeatures: [
          'Longer training (10x more steps)',
          'Removed NSP objective (found ineffective)',
          'Better pre-training data and sampling',
          'Improved downstream task performance',
          'Same architecture, better performance'
        ]
      },
      albert: {
        name: 'ALBERT (2019)',
        description: 'A Lite BERT. Reduced parameters while maintaining performance.',
        keyFeatures: [
          'Factorized embedding parameters',
          'Cross-layer parameter sharing',
          '12M parameters (Base) - 9x smaller than BERT',
          'Faster training and inference',
          'Similar or better performance on benchmarks'
        ]
      },
      distilbert: {
        name: 'DistilBERT (2019)',
        description: 'A distilled version of BERT. 40% smaller, 60% faster, retains 97% performance.',
        keyFeatures: [
          'Knowledge distillation from BERT-base',
          '66M parameters (40% of BERT-base)',
          '60% faster inference',
          'Perfect for production / mobile',
          'Outperforms original BERT in efficiency'
        ]
      },
      debert: {
        name: 'DeBERTa (2020)',
        description: 'Decoding-enhanced BERT with disentangled attention.',
        keyFeatures: [
          'Disentangled attention (content + position separately)',
          'Enhanced mask decoder',
          'Better performance on GLUE & SuperGLUE',
          'More efficient parameter usage',
          'State-of-the-art on many benchmarks'
        ]
      }
    };

    function selectBertVariant(variant) {
      const timelineItems = document.querySelectorAll('.timeline-item');
      timelineItems.forEach(item => item.classList.remove('active'));
      event.target.closest('.timeline-item').classList.add('active');

      const info = bertVariants[variant];
      let html = `
        <h3>${info.name}</h3>
        <p>${info.description}</p>
        <h4 style="color: var(--accent2); margin-top: 1.5rem; margin-bottom: 0.8rem;">Key Features:</h4>
        <ul style="margin-left: 1.5rem;">
      `;

      info.keyFeatures.forEach(feature => {
        html += `<li>${feature}</li>`;
      });

      html += '</ul>';
      document.getElementById('bertVariantInfo').innerHTML = html;
    }

    // Timeline highlight
    function highlightTimeline(idx) {
      const items = document.querySelectorAll('.timeline-item');
      items.forEach(item => item.classList.remove('active'));
      items[idx].classList.add('active');
    }

    // Scroll reveal animation
    function setupScrollReveal() {
      const reveals = document.querySelectorAll('.reveal');
      const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.classList.add('active');
            observer.unobserve(entry.target);
          }
        });
      }, { threshold: 0.1 });

      reveals.forEach(reveal => observer.observe(reveal));
    }

    // Initialize
    window.addEventListener('DOMContentLoaded', () => {
      initMLM();
      renderNSP();
      setupScrollReveal();
      selectBertVariant('bert');
    });
  </script>
</body>
</html>
